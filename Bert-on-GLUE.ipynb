{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39baab6-f824-47a8-9d66-d60d0065aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11011MB, multi_processor_count=68)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"http_proxy\"] = \"http://10.134.231.119:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.134.231.119:7890\"\n",
    "\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9b352-c17e-4bbe-914a-2d59d39dc593",
   "metadata": {},
   "source": [
    "# Bert model\n",
    "pip install transformers\n",
    "\n",
    "Experimental results of bert-base-cased on GLUE \n",
    "| Task  | Metric                       | Result      | Training time | Result (FP16) | Training time (FP16) |\n",
    "|-------|------------------------------|-------------|---------------|---------------|----------------------|\n",
    "| CoLA  | Matthews corr                | 56.53       | 3:17          | 56.78         | 1:41                 |\n",
    "| SST-2 | Accuracy                     | 92.32       | 26:06         | 91.74         | 13:11                |\n",
    "| MRPC  | F1/Accuracy                  | 88.85/84.07 | 2:21          | 88.12/83.58   | 1:10                 |\n",
    "| STS-B | Pearson/Spearman corr.       | 88.64/88.48 | 2:13          | 88.71/88.55   | 1:08                 |\n",
    "| QQP   | Accuracy/F1                  | 90.71/87.49 | 2:22:26       | 90.67/87.43   | 1:11:54              |\n",
    "| MNLI  | Matched acc./Mismatched acc. | 83.91/84.10 | 2:35:23       | 84.04/84.06   | 1:17:06              |\n",
    "| QNLI  | Accuracy                     | 90.66       | 40:57         | 90.96         | 20:16                |\n",
    "| RTE   | Accuracy                     | 65.70       | 57            | 65.34         | 29                   |\n",
    "| WNLI  | Accuracy                     | 56.34       | 24            | 56.34         | 12                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455f33b-82e6-4115-a78d-16696166358a",
   "metadata": {},
   "source": [
    "# Training Bert with PyTorch\n",
    "\n",
    "## 1、Get datasets and tokenizer, processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89662073-b53e-48ae-8239-3d71994ec480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2','idx'])\n",
    "# tokenized_datasets = tokenized_datasets.rename_column('label','labels')  # 实践证明，这一行是不需要的\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# 通过这里的dataloader，每个batch的seq_len可能不同\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'],\n",
    "                              shuffle=True, batch_size=64,\n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'],\n",
    "                             batch_size=64,\n",
    "                             collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69150563-91da-4f15-bca3-ee94f449c956",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# MTL-Bert Model\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification,BertForSequenceClassification\n",
    "import sys\n",
    "sys.path.append('/home/pytorchtest/notebooks/fss')\n",
    "from src.dns_bert import MTLBertForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "if isinstance(pretrained_model,BertForSequenceClassification):\n",
    "    # model = MTLBertForSequenceClassification(pretrained_model.config).to(device)\n",
    "    model = MTLBertForSequenceClassification(model.config,use_dns=True,dns_ratio=0.7).to(device)\n",
    "    model.bert.load_state_dict(pretrained_model.bert.state_dict())\n",
    "else:\n",
    "    raise ValueError(\"pretrained_model must be BertForSequenceClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a705f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)  # num of batches * num of epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,  # scheduler是针对optimizer的lr的\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478533c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################Epoch 1#######################\n",
      "lr = 5e-05\n",
      "Training Epoch [0/3] Iter [0/58]:  loss:8.1469, | Time: 1.9415, Data: 0.1041 | Acc 1/12: 0.6719,Acc 2/12: 0.4219,Acc 3/12: 0.6562,Acc 4/12: 0.6250,Acc 5/12: 0.6406,Acc 6/12: 0.6562,Acc 7/12: 0.3750,Acc 8/12: 0.6562,Acc 9/12: 0.3125,Acc 10/12: 0.6562,Acc 11/12: 0.4375,Acc 12/12: 0.6094,\n",
      "Training Epoch [0/3] Iter [20/58]:  loss:7.4038, | Time: 0.3921, Data: 0.0399 | Acc 1/12: 0.6763,Acc 2/12: 0.6629,Acc 3/12: 0.6756,Acc 4/12: 0.6741,Acc 5/12: 0.6741,Acc 6/12: 0.6868,Acc 7/12: 0.6749,Acc 8/12: 0.6845,Acc 9/12: 0.6763,Acc 10/12: 0.6868,Acc 11/12: 0.6696,Acc 12/12: 0.6823,\n",
      "Training Epoch [0/3] Iter [40/58]:  loss:7.1952, | Time: 0.3566, Data: 0.0382 | Acc 1/12: 0.6787,Acc 2/12: 0.6719,Acc 3/12: 0.6784,Acc 4/12: 0.6806,Acc 5/12: 0.6871,Acc 6/12: 0.7031,Acc 7/12: 0.6955,Acc 8/12: 0.7012,Acc 9/12: 0.6936,Acc 10/12: 0.7005,Acc 11/12: 0.6894,Acc 12/12: 0.6982,\n",
      "Evaluation Epoch [0/3] Iter [0/7]: loss:5.6967, Acc 1/12: 0.6719,Acc 2/12: 0.7344,Acc 3/12: 0.7188,Acc 4/12: 0.7344,Acc 5/12: 0.7500,Acc 6/12: 0.7656,Acc 7/12: 0.7500,Acc 8/12: 0.7656,Acc 9/12: 0.7812,Acc 10/12: 0.7500,Acc 11/12: 0.8125,Acc 12/12: 0.7969,\n",
      "#############################Epoch 2#######################\n",
      "lr = 3.3333333333333335e-05\n",
      "Training Epoch [1/3] Iter [0/58]:  loss:5.5201, | Time: 0.2979, Data: 0.0184 | Acc 1/12: 0.6875,Acc 2/12: 0.7031,Acc 3/12: 0.6719,Acc 4/12: 0.7812,Acc 5/12: 0.7969,Acc 6/12: 0.8438,Acc 7/12: 0.8438,Acc 8/12: 0.8281,Acc 9/12: 0.8281,Acc 10/12: 0.8438,Acc 11/12: 0.8281,Acc 12/12: 0.8125,\n",
      "Training Epoch [1/3] Iter [20/58]:  loss:6.0008, | Time: 0.3190, Data: 0.0346 | Acc 1/12: 0.6786,Acc 2/12: 0.7009,Acc 3/12: 0.7031,Acc 4/12: 0.7344,Acc 5/12: 0.7708,Acc 6/12: 0.7835,Acc 7/12: 0.7842,Acc 8/12: 0.7850,Acc 9/12: 0.7865,Acc 10/12: 0.7902,Acc 11/12: 0.8006,Acc 12/12: 0.7999,\n",
      "Training Epoch [1/3] Iter [40/58]:  loss:5.7859, | Time: 0.3197, Data: 0.0357 | Acc 1/12: 0.6822,Acc 2/12: 0.7035,Acc 3/12: 0.7092,Acc 4/12: 0.7416,Acc 5/12: 0.7797,Acc 6/12: 0.7980,Acc 7/12: 0.7950,Acc 8/12: 0.7976,Acc 9/12: 0.7969,Acc 10/12: 0.8007,Acc 11/12: 0.8152,Acc 12/12: 0.8201,\n",
      "Evaluation Epoch [1/3] Iter [0/7]: loss:4.7254, Acc 1/12: 0.6719,Acc 2/12: 0.7031,Acc 3/12: 0.7188,Acc 4/12: 0.7812,Acc 5/12: 0.8281,Acc 6/12: 0.8750,Acc 7/12: 0.8906,Acc 8/12: 0.8906,Acc 9/12: 0.8906,Acc 10/12: 0.9062,Acc 11/12: 0.9219,Acc 12/12: 0.9375,\n",
      "#############################Epoch 3#######################\n",
      "lr = 1.6666666666666667e-05\n",
      "Training Epoch [2/3] Iter [0/58]:  loss:4.6235, | Time: 0.3220, Data: 0.0187 | Acc 1/12: 0.7031,Acc 2/12: 0.6875,Acc 3/12: 0.7031,Acc 4/12: 0.8281,Acc 5/12: 0.8594,Acc 6/12: 0.8281,Acc 7/12: 0.8281,Acc 8/12: 0.8281,Acc 9/12: 0.8438,Acc 10/12: 0.8438,Acc 11/12: 0.8594,Acc 12/12: 0.8906,\n",
      "Training Epoch [2/3] Iter [20/58]:  loss:4.5647, | Time: 0.3226, Data: 0.0350 | Acc 1/12: 0.6756,Acc 2/12: 0.7106,Acc 3/12: 0.7143,Acc 4/12: 0.8095,Acc 5/12: 0.8490,Acc 6/12: 0.8668,Acc 7/12: 0.8690,Acc 8/12: 0.8735,Acc 9/12: 0.8787,Acc 10/12: 0.8847,Acc 11/12: 0.9025,Acc 12/12: 0.9115,\n",
      "Training Epoch [2/3] Iter [40/58]:  loss:4.5423, | Time: 0.3197, Data: 0.0362 | Acc 1/12: 0.6761,Acc 2/12: 0.7043,Acc 3/12: 0.7161,Acc 4/12: 0.8011,Acc 5/12: 0.8449,Acc 6/12: 0.8689,Acc 7/12: 0.8720,Acc 8/12: 0.8822,Acc 9/12: 0.8857,Acc 10/12: 0.8899,Acc 11/12: 0.9013,Acc 12/12: 0.9104,\n",
      "Evaluation Epoch [2/3] Iter [0/7]: loss:4.5917, Acc 1/12: 0.6719,Acc 2/12: 0.7031,Acc 3/12: 0.7500,Acc 4/12: 0.7812,Acc 5/12: 0.8281,Acc 6/12: 0.8906,Acc 7/12: 0.8750,Acc 8/12: 0.8750,Acc 9/12: 0.8750,Acc 10/12: 0.9062,Acc 11/12: 0.9219,Acc 12/12: 0.9219,\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy_predictions(predictions,references):\n",
    "    return torch.sum(predictions==references)/len(references)\n",
    "def accuracy_logits(logits,references):\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return accuracy_predictions(predictions,references)\n",
    "\n",
    "metric_res = []*model.config.num_hidden_layers\n",
    "print_freq = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  batch_time = AverageMeter()\n",
    "  data_time = AverageMeter()\n",
    "  losses = AverageMeter()\n",
    "  acces = []\n",
    "  for i in range(model.config.num_hidden_layers):\n",
    "      acces.append(AverageMeter())\n",
    "\n",
    "  # training\n",
    "  model.train()\n",
    "  # tbar = tqdm(train_dataloader,unit=\"batch\",desc=\"Epoch %d Training:\"%epoch)\n",
    "  print(f\"#############################Epoch {epoch+1}#######################\")\n",
    "  print(f\"lr = {optimizer.param_groups[0]['lr']}\")\n",
    "  end = time.time()\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "      data_time.update(time.time() - end)\n",
    "      # 要在GPU上训练，需要把数据集都移动到GPU上：\n",
    "      batch = {k:v.to(device) for k,v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss=sum(outputs.loss)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      # measure elapsed time\n",
    "      batch_time.update(time.time() - end)\n",
    "      end = time.time()\n",
    "\n",
    "      losses.update(loss.item(), batch[\"labels\"].size(0))\n",
    "      for i in range(model.config.num_hidden_layers):\n",
    "        acc = accuracy_logits(outputs.logits[i],batch[\"labels\"])\n",
    "        acces[i].update(acc.item(), batch[\"labels\"].size(0))\n",
    "\n",
    "      if idx%print_freq==0:\n",
    "        # 先使用最后一层输出的结果训练看能达到之前的结果吗\n",
    "        msg_info = f\"loss:{losses.avg:.4f}, | Time: {batch_time.avg:.4f}, Data: {data_time.avg:.4f} | \"\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "          msg_info += f\"Acc {i+1}/{model.config.num_hidden_layers}: {acces[i].avg:.4f},\"\n",
    "        print(f\"Training Epoch [{epoch}/{num_epochs}] Iter [{idx}/{len(train_dataloader)}]: \",msg_info)\n",
    "\n",
    "  # evaluation\n",
    "  val_losses = AverageMeter()\n",
    "  val_acces = []\n",
    "  for i in range(model.config.num_hidden_layers):\n",
    "      val_acces.append(AverageMeter())\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(eval_dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():  # evaluation的时候不需要算梯度\n",
    "      outputs = model(**batch)\n",
    "    loss=sum(outputs.loss)\n",
    "    val_losses.update(loss.item(), batch[\"labels\"].size(0))\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "      logits = outputs.logits[i]\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "      acc = accuracy_predictions(predictions,batch[\"labels\"])\n",
    "      val_acces[i].update(acc.item(), batch[\"labels\"].size(0))\n",
    "      # metric_objs[i].add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    if idx%print_freq==0:\n",
    "      msg_info = f\"loss:{val_losses.avg:.4f}, \"\n",
    "      for i in range(model.config.num_hidden_layers):\n",
    "          msg_info += f\"Acc {i+1}/{model.config.num_hidden_layers}: {val_acces[i].avg:.4f},\"\n",
    "      print(f\"Evaluation Epoch [{epoch}/{num_epochs}] Iter [{idx}/{len(eval_dataloader)}]:\",msg_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add2fe29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dd440a40364c4e865f2267e7ffc207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Epoch [2/3] Iter [0/7]: loss:4.5917, Acc 1/12: 0.6719,Acc 2/12: 0.7031,Acc 3/12: 0.7500,Acc 4/12: 0.7812,Acc 5/12: 0.8281,Acc 6/12: 0.8906,Acc 7/12: 0.8750,Acc 8/12: 0.8750,Acc 9/12: 0.8750,Acc 10/12: 0.9062,Acc 11/12: 0.9219,Acc 12/12: 0.9219,\n",
      "Evaluation Epoch [2/3] Iter [1/7]: loss:5.0606, Acc 1/12: 0.7031,Acc 2/12: 0.7266,Acc 3/12: 0.7500,Acc 4/12: 0.8047,Acc 5/12: 0.8281,Acc 6/12: 0.8516,Acc 7/12: 0.8438,Acc 8/12: 0.8516,Acc 9/12: 0.8438,Acc 10/12: 0.8750,Acc 11/12: 0.8828,Acc 12/12: 0.8906,\n",
      "Evaluation Epoch [2/3] Iter [2/7]: loss:5.6142, Acc 1/12: 0.6823,Acc 2/12: 0.7031,Acc 3/12: 0.7292,Acc 4/12: 0.7760,Acc 5/12: 0.7917,Acc 6/12: 0.8125,Acc 7/12: 0.8177,Acc 8/12: 0.8333,Acc 9/12: 0.8281,Acc 10/12: 0.8438,Acc 11/12: 0.8542,Acc 12/12: 0.8594,\n",
      "Evaluation Epoch [2/3] Iter [3/7]: loss:5.7743, Acc 1/12: 0.6953,Acc 2/12: 0.6953,Acc 3/12: 0.7227,Acc 4/12: 0.7773,Acc 5/12: 0.7852,Acc 6/12: 0.7969,Acc 7/12: 0.8008,Acc 8/12: 0.8125,Acc 9/12: 0.8125,Acc 10/12: 0.8203,Acc 11/12: 0.8359,Acc 12/12: 0.8320,\n",
      "Evaluation Epoch [2/3] Iter [4/7]: loss:5.9199, Acc 1/12: 0.6906,Acc 2/12: 0.6937,Acc 3/12: 0.7188,Acc 4/12: 0.7625,Acc 5/12: 0.7875,Acc 6/12: 0.7906,Acc 7/12: 0.7969,Acc 8/12: 0.8094,Acc 9/12: 0.8063,Acc 10/12: 0.8125,Acc 11/12: 0.8313,Acc 12/12: 0.8187,\n",
      "Evaluation Epoch [2/3] Iter [5/7]: loss:6.1069, Acc 1/12: 0.6771,Acc 2/12: 0.6927,Acc 3/12: 0.7161,Acc 4/12: 0.7578,Acc 5/12: 0.7734,Acc 6/12: 0.7812,Acc 7/12: 0.7839,Acc 8/12: 0.7969,Acc 9/12: 0.7943,Acc 10/12: 0.8021,Acc 11/12: 0.8229,Acc 12/12: 0.8099,\n",
      "Evaluation Epoch [2/3] Iter [6/7]: loss:5.9601, Acc 1/12: 0.6838,Acc 2/12: 0.6961,Acc 3/12: 0.7181,Acc 4/12: 0.7647,Acc 5/12: 0.7794,Acc 6/12: 0.7868,Acc 7/12: 0.7892,Acc 8/12: 0.8039,Acc 9/12: 0.8015,Acc 10/12: 0.8088,Acc 11/12: 0.8260,Acc 12/12: 0.8162,\n",
      "[{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}, {'accuracy': 0.696078431372549, 'f1': 0.8062499999999999}, {'accuracy': 0.7181372549019608, 'f1': 0.8211508553654743}, {'accuracy': 0.7647058823529411, 'f1': 0.8481012658227849}, {'accuracy': 0.7794117647058824, 'f1': 0.8529411764705881}, {'accuracy': 0.7867647058823529, 'f1': 0.856198347107438}, {'accuracy': 0.7892156862745098, 'f1': 0.8566666666666668}, {'accuracy': 0.803921568627451, 'f1': 0.8657718120805369}, {'accuracy': 0.8014705882352942, 'f1': 0.8638655462184874}, {'accuracy': 0.8088235294117647, 'f1': 0.8673469387755103}, {'accuracy': 0.8259803921568627, 'f1': 0.8777969018932873}, {'accuracy': 0.8161764705882353, 'f1': 0.8709122203098106}]\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "metric_objs = [evaluate.load(\"glue\", \"mrpc\") for _ in range(model.config.num_hidden_layers)]\n",
    "val_losses = AverageMeter()\n",
    "val_acces = []\n",
    "print_freq = 1\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    val_acces.append(AverageMeter())\n",
    "model.eval()\n",
    "for idx,batch in enumerate(eval_dataloader):\n",
    "  batch = {k: v.to(device) for k, v in batch.items()}\n",
    "  with torch.no_grad():  # evaluation的时候不需要算梯度\n",
    "    outputs = model(**batch)\n",
    "  loss=sum(outputs.loss)\n",
    "  val_losses.update(loss.item(), batch[\"labels\"].size(0))\n",
    "  for i in range(model.config.num_hidden_layers):\n",
    "    logits = outputs.logits[i]\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    acc = accuracy_predictions(predictions,batch[\"labels\"])\n",
    "    val_acces[i].update(acc.item(), batch[\"labels\"].size(0))\n",
    "    metric_objs[i].add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "  if idx%print_freq==0:\n",
    "    msg_info = f\"loss:{val_losses.avg:.4f}, \"\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        msg_info += f\"Acc {i+1}/{model.config.num_hidden_layers}: {val_acces[i].avg:.4f},\"\n",
    "    print(f\"Evaluation Epoch [{epoch}/{num_epochs}] Iter [{idx}/{len(eval_dataloader)}]:\",msg_info)\n",
    "\n",
    "for i in range(len(metric_objs)):\n",
    "  metric_res.append(metric_objs[i].compute())\n",
    "\n",
    "bert_base_base_res_on_mrpc = [f\"{i['f1']*100:.2f}/{i['accuracy']*100:.2f}\" for i in metric_res]\n",
    "for i in bert_base_base_res_on_mrpc:\n",
    "  print(i,\"|\",end=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63f7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
