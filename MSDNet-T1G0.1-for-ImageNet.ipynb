{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb24ef73-b23b-4f65-9498-2154a1b90964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24575MB, multi_processor_count=82)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743031d-3749-463c-b98b-549967a4bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../packages/autoqnn/\")\n",
    "# import autoqnn\n",
    "from dns_msdnet import msdnet_cifar100, msdnet_imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163249-ee08-4701-8bc6-ba8d48adb76e",
   "metadata": {},
   "source": [
    "# DNS MSDNet \n",
    "测试DNS在MSDNet上的效果如何，首先是在Cifar100数据集上的效果，先测试一下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d6e195-fb46-47af-82c1-3139f2ae75b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building network of steps: \n",
      "[4, 4, 4, 4, 4] 20\n",
      " ********************** Block 1  **********************\n",
      "|\t\tinScales 4 outScales 4 inChannels 32 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 48 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 2  **********************\n",
      "|\t\tinScales 4 outScales 4 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 3 inChannels 112 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 128, outChannels 64\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 3  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 112 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 2 inChannels 128 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 144, outChannels 72\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 72 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 4  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 88 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 104 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 120 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 1 inChannels 136 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 152, outChannels 76\t|\n",
      "\n",
      " ********************** Block 5  **********************\n",
      "|\t\tinScales 1 outScales 1 inChannels 76 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 92 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 108 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 124 outChannels 16\t\t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = msdnet_imagenet().cuda()\n",
    "model_name = \"msdnet_imagenet_gamma-0.1-T-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7f7b5c-80dd-4dad-8264-730a07f22b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = load_checkpoint('./models/msdnet_imagenet-cifar100/save_models')\n",
    "# model.load_state_dict(state_dict[\"state_dict\"])\n",
    "# state_dict[\"state_dict\"]\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022059f-136a-4ce6-b783-62f47acf1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "import datasets\n",
    "data_path=\"/open_datasets/imagenet\"\n",
    "trainloader,testloader = datasets.imagenet.get_dataset(\n",
    "    data_path=data_path,batch_size=256, workers=4, parse_type=\"torch\",prefetch=True)\n",
    "data_name=\"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45022ef-1dc0-4e75-96e0-da374489af54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, temperature, gamma, nBlocks):\n",
    "        super(KDLoss, self).__init__()\n",
    "        \n",
    "        self.kld_loss = nn.KLDivLoss().cuda()\n",
    "        self.ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "        self.softmax = nn.Softmax(dim=1).cuda()\n",
    "\n",
    "        self.T = temperature\n",
    "        self.gamma = gamma\n",
    "        self.nBlocks = nBlocks\n",
    "\n",
    "    def loss_fn_kd(self, outputs, targets, soft_targets):\n",
    "        loss = self.ce_loss(outputs[-1], targets)\n",
    "        T = self.T\n",
    "        for i in range(self.nBlocks - 1):\n",
    "            _ce = (1. - self.gamma) * self.ce_loss(outputs[i], targets)\n",
    "            _kld = self.kld_loss(self.log_softmax(outputs[i] / T), self.softmax(soft_targets.detach() / T)) * self.gamma * T * T\n",
    "            loss = loss + _ce + _kld\n",
    "        return loss\n",
    "\n",
    "def train(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq=10):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    running_lr = None\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=i,\n",
    "                                  nBatch=len(train_loader), method=\"multistep\")\n",
    "        # measure data loading time\n",
    "        if running_lr is None:\n",
    "            running_lr = lr\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output, middle_feas = model(input_var)\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "\n",
    "        loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        for j in range(len(output)):\n",
    "            acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "            top1[j].update(acc1.item(), input.size(0))\n",
    "            top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.avg:.3f}\\t'\n",
    "                  'Data {data_time.avg:.3f}\\t'\n",
    "                  'Loss {loss.val:.4f}\\t'\n",
    "                  'Acc@1 {top1.val:.4f}\\t'\n",
    "                  'Acc@5 {top5.val:.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader),\n",
    "                    batch_time=batch_time, data_time=data_time,\n",
    "                    loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg, running_lr\n",
    "\n",
    "def validate(val_loader, model, kd_loss,print_freq=10):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output,_ = model(input_var)\n",
    "            if not isinstance(output, list):\n",
    "                output = [output]\n",
    "\n",
    "            loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "\n",
    "            # measure error and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "\n",
    "            for j in range(len(output)):\n",
    "                acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "                top1[j].update(acc1.item(), input.size(0))\n",
    "                top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.avg:.3f}\\t'\n",
    "                      'Data {data_time.avg:.3f}\\t'\n",
    "                      'Loss {loss.val:.4f}\\t'\n",
    "                      'Acc@1 {top1.val:.4f}\\t'\n",
    "                      'Acc@5 {top5.val:.4f}'.format(\n",
    "                        i + 1, len(val_loader),\n",
    "                        batch_time=batch_time, data_time=data_time,\n",
    "                        loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "                # break\n",
    "    for j in range(model.nBlocks):\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1[j], top5=top5[j]))\n",
    "        \"\"\"\n",
    "        print('Exit {}\\t'\n",
    "              'Err@1 {:.4f}\\t'\n",
    "              'Err@5 {:.4f}'.format(\n",
    "              j, top1[j].avg, top5[j].avg))\n",
    "        \"\"\"\n",
    "    # print(' * Err@1 {top1.avg:.3f} Err@5 {top5.avg:.3f}'.format(top1=top1[-1], top5=top5[-1]))\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best, filename, result):\n",
    "    print(save_path)\n",
    "    result_filename = os.path.join(save_path, 'scores.tsv')\n",
    "    model_dir = os.path.join(save_path, 'save_models')\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    model_filename = os.path.join(model_dir, filename)\n",
    "    best_filename = os.path.join(model_dir, 'model_best.pth.tar')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"=> saving checkpoint '{}'\".format(model_filename))\n",
    "\n",
    "    torch.save(state, model_filename)\n",
    "\n",
    "    with open(result_filename, 'w') as f:\n",
    "        print('\\n'.join(result), file=f)\n",
    "\n",
    "    with open(latest_filename, 'w') as fout:\n",
    "        fout.write(model_filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_filename, best_filename)\n",
    "\n",
    "    print(\"=> saved checkpoint '{}'\".format(model_filename))\n",
    "    return\n",
    "\n",
    "def load_checkpoint(save_path):\n",
    "    model_dir = save_path\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    if os.path.exists(latest_filename):\n",
    "        with open(latest_filename, 'r') as fin:\n",
    "            model_filename = fin.readlines()[0]\n",
    "    else:\n",
    "        return None\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_filename))\n",
    "    state = torch.load(model_filename)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(model_filename))\n",
    "    return state\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the error@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        # res.append(100.0 - correct_k.mul_(100.0 / batch_size))\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "import math\n",
    "def adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=None,\n",
    "                         nBatch=None, method='multistep'):\n",
    "    if method == 'cosine':\n",
    "        T_total = epochs * nBatch\n",
    "        T_cur = (epoch % epochs) * nBatch + batch\n",
    "        lr = 0.5 * init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n",
    "    elif method == 'multistep':\n",
    "        data = \"cifar100\"\n",
    "        if data.startswith('cifar'):\n",
    "            lr, decay_rate = init_lr, 0.1\n",
    "            if epoch >= epochs * 0.75:\n",
    "                lr *= decay_rate ** 2\n",
    "            elif epoch >= epochs * 0.5:\n",
    "                lr *= decay_rate\n",
    "        else:\n",
    "            lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cb254-dd16-4641-9fa5-c8f292286396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code\n",
    "init_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "epochs = 70\n",
    "# for imagenet, T = 1, gamma = 0.1\n",
    "# default is T = 3, gamma = 0.9\n",
    "temperature = 1.0\n",
    "gamma = 0.1\n",
    "nBlocks = model.nBlocks\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "kd_loss = KDLoss(temperature, gamma, nBlocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f8e69-1a3b-42da-bc29-2b5df2aa34d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = ['epoch\\tlr\\ttrain_loss\\tval_loss\\ttrain_acc1'\n",
    "              '\\tval_acc1\\ttrain_acc5\\tval_acc5']\n",
    "\n",
    "import sys\n",
    "import time\n",
    "# 将程序的输出重定向到文件  \n",
    "file_path = f'./training_logs/{model_name}-on-{data_name}-lr-{init_lr}-wd-{weight_decay}-epoch-{epochs}.txt'  \n",
    "sys.stdout = open(file_path, 'w')\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"#################epoch {epoch+1}#################\")\n",
    "    print(\"lr=%.6f \\n\"%(optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    # train for one epoch\n",
    "    train_loss, train_acc1, train_acc5, lr = train(trainloader, model, \n",
    "                                                   kd_loss, optimizer, \n",
    "                                                   epoch,epochs,init_lr,print_freq=100)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_acc1, val_acc5 = validate(testloader, model, kd_loss,print_freq=100)\n",
    "\n",
    "    # save scores to a tsv file, rewrite the whole file to prevent\n",
    "    # accidental deletion\n",
    "    scores.append(('{}\\t{:.3f}' + '\\t{:.4f}' * 6)\n",
    "                  .format(epoch, lr, train_loss, val_loss,\n",
    "                          train_acc1, val_acc1, train_acc5, val_acc5))\n",
    "\n",
    "    is_best = val_acc1 > best_acc1\n",
    "    if is_best:\n",
    "        best_acc1 = val_acc1\n",
    "        best_epoch = epoch\n",
    "        print('Best var_acc1 {}'.format(best_acc1))\n",
    "\n",
    "        save_path = f\"./models/{model_name}-on-{{data_name}}/\"\n",
    "        model_filename = f'training_state_dict.pth.tar'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': model_name,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, save_path, is_best, model_filename, scores)\n",
    "\n",
    "print('Best val_acc1: {:.4f} at epoch {}'.format(best_acc1, best_epoch))\n",
    "\n",
    "# Save the trained parameters to disk\n",
    "# best_loss = np.min(val_loss)\n",
    "# best_metric = np.max(val_metrics）\n",
    "save_file_name = f\"./models/{model_name}-on-{data_name}-top1-{best_acc1}.pth\"\n",
    "torch.save(model.state_dict(),save_file_name)\n",
    "print(f\"save model weight to {save_file_name}\")\n",
    "# 关闭文件  \n",
    "sys.stdout.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8fe97-e117-4d95-9398-f8b1b374d6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(trainloader, model, kd_loss, optimizer, 1,epochs,init_lr,print_freq=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
