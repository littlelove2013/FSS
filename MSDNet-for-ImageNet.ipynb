{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb24ef73-b23b-4f65-9498-2154a1b90964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24259MB, multi_processor_count=82)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743031d-3749-463c-b98b-549967a4bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../packages/autoqnn/\")\n",
    "import autoqnn\n",
    "from dns_msdnet import msdnet_cifar100, msdnet_imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163249-ee08-4701-8bc6-ba8d48adb76e",
   "metadata": {},
   "source": [
    "# DNS MSDNet \n",
    "测试DNS在MSDNet上的效果如何，首先是在Cifar100数据集上的效果，先测试一下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d6e195-fb46-47af-82c1-3139f2ae75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building network of steps: \n",
      "[4, 4, 4, 4, 4] 20\n",
      " ********************** Block 1  **********************\n",
      "|\t\tinScales 4 outScales 4 inChannels 32 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 48 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 4 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 2  **********************\n",
      "|\t\tinScales 4 outScales 4 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 3 inChannels 112 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 128, outChannels 64\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 3  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 112 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 2 inChannels 128 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 144, outChannels 72\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 72 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 4  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 88 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 104 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 120 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 1 inChannels 136 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 152, outChannels 76\t|\n",
      "\n",
      " ********************** Block 5  **********************\n",
      "|\t\tinScales 1 outScales 1 inChannels 76 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 92 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 108 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 124 outChannels 16\t\t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = msdnet_imagenet().cuda()\n",
    "model_name = \"msdnet_imagenet_gamma-0.9-T-3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7f7b5c-80dd-4dad-8264-730a07f22b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = load_checkpoint('./models/msdnet_imagenet-cifar100/save_models')\n",
    "# model.load_state_dict(state_dict[\"state_dict\"])\n",
    "# state_dict[\"state_dict\"]\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b022059f-136a-4ce6-b783-62f47acf1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "import datasets\n",
    "data_path=\"/open_datasets/imagenet\"\n",
    "trainloader,testloader = datasets.imagenet.get_dataset(\n",
    "    data_path=data_path,batch_size=256, workers=8, parse_type=\"torch\",prefetch=True)\n",
    "data_name=\"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45022ef-1dc0-4e75-96e0-da374489af54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, temperature, gamma, nBlocks):\n",
    "        super(KDLoss, self).__init__()\n",
    "        \n",
    "        self.kld_loss = nn.KLDivLoss().cuda()\n",
    "        self.ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "        self.softmax = nn.Softmax(dim=1).cuda()\n",
    "\n",
    "        self.T = temperature\n",
    "        self.gamma = gamma\n",
    "        self.nBlocks = nBlocks\n",
    "\n",
    "    def loss_fn_kd(self, outputs, targets, soft_targets):\n",
    "        loss = self.ce_loss(outputs[-1], targets)\n",
    "        T = self.T\n",
    "        for i in range(self.nBlocks - 1):\n",
    "            _ce = (1. - self.gamma) * self.ce_loss(outputs[i], targets)\n",
    "            _kld = self.kld_loss(self.log_softmax(outputs[i] / T), self.softmax(soft_targets.detach() / T)) * self.gamma * T * T\n",
    "            loss = loss + _ce + _kld\n",
    "        return loss\n",
    "\n",
    "def train(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq=10):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    running_lr = None\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=i,\n",
    "                                  nBatch=len(train_loader), method=\"multistep\")\n",
    "        # measure data loading time\n",
    "        if running_lr is None:\n",
    "            running_lr = lr\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output, middle_feas = model(input_var)\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "\n",
    "        loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        for j in range(len(output)):\n",
    "            acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "            top1[j].update(acc1.item(), input.size(0))\n",
    "            top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.avg:.3f}\\t'\n",
    "                  'Data {data_time.avg:.3f}\\t'\n",
    "                  'Loss {loss.val:.4f}\\t'\n",
    "                  'Acc@1 {top1.val:.4f}\\t'\n",
    "                  'Acc@5 {top5.val:.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader),\n",
    "                    batch_time=batch_time, data_time=data_time,\n",
    "                    loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg, running_lr\n",
    "\n",
    "def validate(val_loader, model, kd_loss,print_freq=10):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output,_ = model(input_var)\n",
    "            if not isinstance(output, list):\n",
    "                output = [output]\n",
    "\n",
    "            loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "\n",
    "            # measure error and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "\n",
    "            for j in range(len(output)):\n",
    "                acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "                top1[j].update(acc1.item(), input.size(0))\n",
    "                top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.avg:.3f}\\t'\n",
    "                      'Data {data_time.avg:.3f}\\t'\n",
    "                      'Loss {loss.val:.4f}\\t'\n",
    "                      'Acc@1 {top1.val:.4f}\\t'\n",
    "                      'Acc@5 {top5.val:.4f}'.format(\n",
    "                        i + 1, len(val_loader),\n",
    "                        batch_time=batch_time, data_time=data_time,\n",
    "                        loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "                # break\n",
    "    for j in range(model.nBlocks):\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1[j], top5=top5[j]))\n",
    "        \"\"\"\n",
    "        print('Exit {}\\t'\n",
    "              'Err@1 {:.4f}\\t'\n",
    "              'Err@5 {:.4f}'.format(\n",
    "              j, top1[j].avg, top5[j].avg))\n",
    "        \"\"\"\n",
    "    # print(' * Err@1 {top1.avg:.3f} Err@5 {top5.avg:.3f}'.format(top1=top1[-1], top5=top5[-1]))\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best, filename, result):\n",
    "    print(save_path)\n",
    "    result_filename = os.path.join(save_path, 'scores.tsv')\n",
    "    model_dir = os.path.join(save_path, 'save_models')\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    model_filename = os.path.join(model_dir, filename)\n",
    "    best_filename = os.path.join(model_dir, 'model_best.pth.tar')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"=> saving checkpoint '{}'\".format(model_filename))\n",
    "\n",
    "    torch.save(state, model_filename)\n",
    "\n",
    "    with open(result_filename, 'w') as f:\n",
    "        print('\\n'.join(result), file=f)\n",
    "\n",
    "    with open(latest_filename, 'w') as fout:\n",
    "        fout.write(model_filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_filename, best_filename)\n",
    "\n",
    "    print(\"=> saved checkpoint '{}'\".format(model_filename))\n",
    "    return\n",
    "\n",
    "def load_checkpoint(save_path):\n",
    "    model_dir = save_path\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    if os.path.exists(latest_filename):\n",
    "        with open(latest_filename, 'r') as fin:\n",
    "            model_filename = fin.readlines()[0]\n",
    "    else:\n",
    "        return None\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_filename))\n",
    "    state = torch.load(model_filename)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(model_filename))\n",
    "    return state\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the error@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        # res.append(100.0 - correct_k.mul_(100.0 / batch_size))\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "import math\n",
    "def adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=None,\n",
    "                         nBatch=None, method='multistep'):\n",
    "    if method == 'cosine':\n",
    "        T_total = epochs * nBatch\n",
    "        T_cur = (epoch % epochs) * nBatch + batch\n",
    "        lr = 0.5 * init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n",
    "    elif method == 'multistep':\n",
    "        data = \"cifar100\"\n",
    "        if data.startswith('cifar'):\n",
    "            lr, decay_rate = init_lr, 0.1\n",
    "            if epoch >= epochs * 0.75:\n",
    "                lr *= decay_rate ** 2\n",
    "            elif epoch >= epochs * 0.5:\n",
    "                lr *= decay_rate\n",
    "        else:\n",
    "            lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557cb254-dd16-4641-9fa5-c8f292286396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code\n",
    "init_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "epochs = 70\n",
    "# for imagenet, T = 1, gamma = 0.1\n",
    "# default is T = 3, gamma = 0.9\n",
    "temperature = 3.0\n",
    "gamma = 0.9\n",
    "nBlocks = model.nBlocks\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "kd_loss = KDLoss(temperature, gamma, nBlocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f8e69-1a3b-42da-bc29-2b5df2aa34d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = ['epoch\\tlr\\ttrain_loss\\tval_loss\\ttrain_acc1'\n",
    "              '\\tval_acc1\\ttrain_acc5\\tval_acc5']\n",
    "\n",
    "import sys\n",
    "import time\n",
    "# 将程序的输出重定向到文件  \n",
    "file_path = f'./training_logs/{model_name}-on-{data_name}-lr-{init_lr}-wd-{weight_decay}-epoch-{epochs}.txt'  \n",
    "sys.stdout = open(file_path, 'w')\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"#################epoch {epoch+1}#################\")\n",
    "    print(\"lr=%.6f \\n\"%(optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    # train for one epoch\n",
    "    train_loss, train_acc1, train_acc5, lr = train(trainloader, model, \n",
    "                                                   kd_loss, optimizer, \n",
    "                                                   epoch,epochs,init_lr,print_freq=100)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_acc1, val_acc5 = validate(testloader, model, kd_loss,print_freq=100)\n",
    "\n",
    "    # save scores to a tsv file, rewrite the whole file to prevent\n",
    "    # accidental deletion\n",
    "    scores.append(('{}\\t{:.3f}' + '\\t{:.4f}' * 6)\n",
    "                  .format(epoch, lr, train_loss, val_loss,\n",
    "                          train_acc1, val_acc1, train_acc5, val_acc5))\n",
    "\n",
    "    is_best = val_acc1 > best_acc1\n",
    "    if is_best:\n",
    "        best_acc1 = val_acc1\n",
    "        best_epoch = epoch\n",
    "        print('Best var_acc1 {}'.format(best_acc1))\n",
    "\n",
    "        save_path = f\"./models/{model_name}-on-{{data_name}}/\"\n",
    "        model_filename = f'training_state_dict.pth.tar'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': model_name,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, save_path, is_best, model_filename, scores)\n",
    "\n",
    "print('Best val_acc1: {:.4f} at epoch {}'.format(best_acc1, best_epoch))\n",
    "\n",
    "# Save the trained parameters to disk\n",
    "# best_loss = np.min(val_loss)\n",
    "# best_metric = np.max(val_metrics）\n",
    "save_file_name = f\"./models/{model_name}-on-{{data_name}}-top1-{best_acc1}.pth\"\n",
    "torch.save(model.state_dict(),save_file_name)\n",
    "print(f\"save model weight to {save_file_name}\")\n",
    "# 关闭文件  \n",
    "sys.stdout.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a8fe97-e117-4d95-9398-f8b1b374d6f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/5005]\tTime 12.559\tData 6.861\tLoss 9.7580\tAcc@1 0.0000\tAcc@5 0.0000\n",
      "Epoch: [1][11/5005]\tTime 1.416\tData 0.624\tLoss 9.7424\tAcc@1 0.0000\tAcc@5 0.7812\n",
      "Epoch: [1][21/5005]\tTime 0.893\tData 0.327\tLoss 9.7428\tAcc@1 0.0000\tAcc@5 0.3906\n",
      "Epoch: [1][31/5005]\tTime 0.716\tData 0.241\tLoss 9.7149\tAcc@1 0.3906\tAcc@5 0.7812\n",
      "Epoch: [1][41/5005]\tTime 0.669\tData 0.245\tLoss 9.6828\tAcc@1 0.0000\tAcc@5 1.5625\n",
      "Epoch: [1][51/5005]\tTime 0.644\tData 0.250\tLoss 9.6174\tAcc@1 0.3906\tAcc@5 1.1719\n",
      "Epoch: [1][61/5005]\tTime 0.626\tData 0.254\tLoss 9.6365\tAcc@1 0.3906\tAcc@5 1.9531\n",
      "Epoch: [1][71/5005]\tTime 0.609\tData 0.252\tLoss 9.6197\tAcc@1 1.1719\tAcc@5 1.5625\n",
      "Epoch: [1][81/5005]\tTime 0.595\tData 0.251\tLoss 9.5954\tAcc@1 0.3906\tAcc@5 0.7812\n",
      "Epoch: [1][91/5005]\tTime 0.585\tData 0.251\tLoss 9.5390\tAcc@1 0.3906\tAcc@5 1.5625\n",
      "Epoch: [1][101/5005]\tTime 0.579\tData 0.254\tLoss 9.6047\tAcc@1 0.3906\tAcc@5 1.9531\n",
      "Epoch: [1][111/5005]\tTime 0.576\tData 0.255\tLoss 9.5207\tAcc@1 0.3906\tAcc@5 0.3906\n",
      "Epoch: [1][121/5005]\tTime 0.570\tData 0.253\tLoss 9.4660\tAcc@1 0.3906\tAcc@5 2.7344\n",
      "Epoch: [1][131/5005]\tTime 0.567\tData 0.253\tLoss 9.4135\tAcc@1 0.7812\tAcc@5 3.1250\n",
      "Epoch: [1][141/5005]\tTime 0.569\tData 0.257\tLoss 9.4334\tAcc@1 0.3906\tAcc@5 2.3438\n",
      "Epoch: [1][151/5005]\tTime 0.563\tData 0.253\tLoss 9.3231\tAcc@1 1.1719\tAcc@5 3.1250\n",
      "Epoch: [1][161/5005]\tTime 0.558\tData 0.251\tLoss 9.3332\tAcc@1 0.7812\tAcc@5 2.3438\n",
      "Epoch: [1][171/5005]\tTime 0.557\tData 0.252\tLoss 9.2973\tAcc@1 0.3906\tAcc@5 3.1250\n",
      "Epoch: [1][181/5005]\tTime 0.555\tData 0.251\tLoss 9.3128\tAcc@1 0.0000\tAcc@5 3.1250\n",
      "Epoch: [1][191/5005]\tTime 0.550\tData 0.248\tLoss 9.2457\tAcc@1 0.7812\tAcc@5 4.2969\n",
      "Epoch: [1][201/5005]\tTime 0.549\tData 0.247\tLoss 9.2279\tAcc@1 0.3906\tAcc@5 2.7344\n",
      "Epoch: [1][211/5005]\tTime 0.545\tData 0.245\tLoss 9.2612\tAcc@1 0.7812\tAcc@5 2.7344\n",
      "Epoch: [1][221/5005]\tTime 0.547\tData 0.248\tLoss 9.2185\tAcc@1 0.0000\tAcc@5 3.9062\n",
      "Epoch: [1][231/5005]\tTime 0.543\tData 0.246\tLoss 9.1143\tAcc@1 0.0000\tAcc@5 5.4688\n",
      "Epoch: [1][241/5005]\tTime 0.542\tData 0.246\tLoss 9.1092\tAcc@1 0.7812\tAcc@5 4.2969\n",
      "Epoch: [1][251/5005]\tTime 0.540\tData 0.246\tLoss 9.0957\tAcc@1 0.3906\tAcc@5 3.1250\n",
      "Epoch: [1][261/5005]\tTime 0.542\tData 0.249\tLoss 9.1020\tAcc@1 0.7812\tAcc@5 3.1250\n",
      "Epoch: [1][271/5005]\tTime 0.540\tData 0.248\tLoss 9.1159\tAcc@1 0.3906\tAcc@5 3.9062\n",
      "Epoch: [1][281/5005]\tTime 0.540\tData 0.250\tLoss 9.0365\tAcc@1 1.9531\tAcc@5 3.1250\n",
      "Epoch: [1][291/5005]\tTime 0.538\tData 0.248\tLoss 8.9109\tAcc@1 1.5625\tAcc@5 5.4688\n",
      "Epoch: [1][301/5005]\tTime 0.538\tData 0.249\tLoss 8.9696\tAcc@1 0.7812\tAcc@5 2.7344\n",
      "Epoch: [1][311/5005]\tTime 0.536\tData 0.247\tLoss 8.8467\tAcc@1 1.9531\tAcc@5 7.4219\n",
      "Epoch: [1][321/5005]\tTime 0.536\tData 0.247\tLoss 8.9923\tAcc@1 0.7812\tAcc@5 4.6875\n",
      "Epoch: [1][331/5005]\tTime 0.534\tData 0.246\tLoss 8.9335\tAcc@1 0.3906\tAcc@5 5.8594\n",
      "Epoch: [1][341/5005]\tTime 0.533\tData 0.246\tLoss 8.8577\tAcc@1 1.1719\tAcc@5 3.5156\n",
      "Epoch: [1][351/5005]\tTime 0.532\tData 0.245\tLoss 8.9338\tAcc@1 0.7812\tAcc@5 3.1250\n",
      "Epoch: [1][361/5005]\tTime 0.534\tData 0.248\tLoss 8.7635\tAcc@1 1.1719\tAcc@5 4.6875\n",
      "Epoch: [1][371/5005]\tTime 0.532\tData 0.245\tLoss 8.7447\tAcc@1 1.9531\tAcc@5 4.2969\n",
      "Epoch: [1][381/5005]\tTime 0.530\tData 0.243\tLoss 8.7295\tAcc@1 1.5625\tAcc@5 7.8125\n",
      "Epoch: [1][391/5005]\tTime 0.530\tData 0.243\tLoss 8.7614\tAcc@1 0.0000\tAcc@5 5.8594\n",
      "Epoch: [1][401/5005]\tTime 0.533\tData 0.246\tLoss 8.6546\tAcc@1 2.7344\tAcc@5 8.9844\n",
      "Epoch: [1][411/5005]\tTime 0.533\tData 0.246\tLoss 8.8583\tAcc@1 0.7812\tAcc@5 4.2969\n",
      "Epoch: [1][421/5005]\tTime 0.530\tData 0.243\tLoss 8.5754\tAcc@1 2.3438\tAcc@5 10.1562\n",
      "Epoch: [1][431/5005]\tTime 0.529\tData 0.241\tLoss 8.6148\tAcc@1 3.1250\tAcc@5 9.3750\n",
      "Epoch: [1][441/5005]\tTime 0.531\tData 0.243\tLoss 8.7378\tAcc@1 1.1719\tAcc@5 7.8125\n",
      "Epoch: [1][451/5005]\tTime 0.530\tData 0.242\tLoss 8.7816\tAcc@1 1.5625\tAcc@5 5.8594\n",
      "Epoch: [1][461/5005]\tTime 0.528\tData 0.239\tLoss 8.6026\tAcc@1 1.9531\tAcc@5 8.2031\n",
      "Epoch: [1][471/5005]\tTime 0.527\tData 0.239\tLoss 8.5214\tAcc@1 3.5156\tAcc@5 7.8125\n",
      "Epoch: [1][481/5005]\tTime 0.529\tData 0.240\tLoss 8.6819\tAcc@1 1.9531\tAcc@5 7.0312\n",
      "Epoch: [1][491/5005]\tTime 0.525\tData 0.236\tLoss 8.5074\tAcc@1 1.1719\tAcc@5 10.1562\n",
      "Epoch: [1][501/5005]\tTime 0.522\tData 0.233\tLoss 8.6220\tAcc@1 1.1719\tAcc@5 6.2500\n",
      "Epoch: [1][511/5005]\tTime 0.523\tData 0.233\tLoss 8.5971\tAcc@1 3.1250\tAcc@5 7.4219\n",
      "Epoch: [1][521/5005]\tTime 0.528\tData 0.238\tLoss 8.6020\tAcc@1 1.5625\tAcc@5 7.8125\n",
      "Epoch: [1][531/5005]\tTime 0.527\tData 0.237\tLoss 8.4745\tAcc@1 1.5625\tAcc@5 7.8125\n",
      "Epoch: [1][541/5005]\tTime 0.527\tData 0.237\tLoss 8.4623\tAcc@1 2.7344\tAcc@5 7.8125\n",
      "Epoch: [1][551/5005]\tTime 0.526\tData 0.236\tLoss 8.5095\tAcc@1 0.7812\tAcc@5 7.4219\n",
      "Epoch: [1][561/5005]\tTime 0.528\tData 0.237\tLoss 8.6411\tAcc@1 1.9531\tAcc@5 9.7656\n",
      "Epoch: [1][571/5005]\tTime 0.527\tData 0.237\tLoss 8.3258\tAcc@1 1.1719\tAcc@5 8.9844\n",
      "Epoch: [1][581/5005]\tTime 0.526\tData 0.236\tLoss 8.2907\tAcc@1 1.9531\tAcc@5 15.2344\n",
      "Epoch: [1][591/5005]\tTime 0.526\tData 0.236\tLoss 8.5422\tAcc@1 1.9531\tAcc@5 6.6406\n",
      "Epoch: [1][601/5005]\tTime 0.527\tData 0.237\tLoss 8.3595\tAcc@1 1.1719\tAcc@5 7.0312\n",
      "Epoch: [1][611/5005]\tTime 0.526\tData 0.237\tLoss 8.3633\tAcc@1 1.5625\tAcc@5 7.4219\n",
      "Epoch: [1][621/5005]\tTime 0.525\tData 0.235\tLoss 8.3343\tAcc@1 3.9062\tAcc@5 10.5469\n",
      "Epoch: [1][631/5005]\tTime 0.524\tData 0.234\tLoss 8.1233\tAcc@1 3.9062\tAcc@5 14.0625\n",
      "Epoch: [1][641/5005]\tTime 0.525\tData 0.235\tLoss 8.2750\tAcc@1 3.1250\tAcc@5 10.5469\n",
      "Epoch: [1][651/5005]\tTime 0.524\tData 0.234\tLoss 8.4087\tAcc@1 2.3438\tAcc@5 10.1562\n",
      "Epoch: [1][661/5005]\tTime 0.523\tData 0.233\tLoss 8.1622\tAcc@1 3.9062\tAcc@5 10.5469\n",
      "Epoch: [1][671/5005]\tTime 0.525\tData 0.235\tLoss 8.3120\tAcc@1 1.5625\tAcc@5 8.9844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkd_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43minit_lr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq)\u001b[0m\n\u001b[1;32m     60\u001b[0m     output \u001b[38;5;241m=\u001b[39m [output]\n\u001b[1;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m kd_loss\u001b[38;5;241m.\u001b[39mloss_fn_kd(output, target_var, output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 63\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output)):\n\u001b[1;32m     65\u001b[0m     acc1, acc5 \u001b[38;5;241m=\u001b[39m accuracy(output[j]\u001b[38;5;241m.\u001b[39mdata, target, topk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(trainloader, model, kd_loss, optimizer, 1,epochs,init_lr,print_freq=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
