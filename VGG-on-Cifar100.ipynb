{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11011MB, multi_processor_count=68)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf6735f-7164-4177-a829-98033ecfee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(f\"{os.environ['HOME']}/notebooks/fss\")\n",
    "from src.datasets import cifar\n",
    "# torchvision数据集的输出是在[0, 1]范围内的PILImage图片。\n",
    "# 我们此处使用归一化的方法将其转化为Tensor，数据范围为[-1, 1]\n",
    "# !wget -P /data/gongcheng/models/torch/weights/ https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# torch_weights_path=\"/data/gongcheng/models/torch/weights\"\n",
    "torch_weights_path=f\"{os.environ['HOME']}/models/torch/weights\"\n",
    "data_root=\"~/datasets/cifar100\"\n",
    "dataset_name = 'cifar100'\n",
    "trainloader,testloader,classes = cifar.get_cifar_dataloader(root=data_root,dataset=dataset_name,autoaugment=True,num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6943ce-fb60-4bbe-af84-6e270e260f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.dns_vgg import MTL_VGG7_64_plane_base\n",
    "import torch\n",
    "from torch import nn\n",
    "class MTL_VGG7_64_plane_dns(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channel_num=64,\n",
    "                 num_classes=10,\n",
    "                 use_dns=True,\n",
    "                 use_fr=True,\n",
    "                 dns_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dns_ratio = dns_ratio\n",
    "        self.plus_fea_num = int(channel_num * dns_ratio)\n",
    "        self.sub_fea_num = channel_num - self.plus_fea_num\n",
    "\n",
    "        # block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU(True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "\n",
    "        self.classifer1 = nn.Sequential(\n",
    "                    nn.AvgPool2d(32),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(64, num_classes)\n",
    "                )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU(True)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.classifer2 = nn.Sequential(\n",
    "                    nn.AvgPool2d(16),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(64, num_classes)\n",
    "                )\n",
    "\n",
    "        # block 2\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU(True)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.classifer3 = nn.Sequential(\n",
    "                    nn.AvgPool2d(16),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(128, num_classes)\n",
    "                )\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU(True)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.classifer4 = nn.Sequential(\n",
    "                    nn.AvgPool2d(8),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(128, num_classes)\n",
    "                )\n",
    "        \n",
    "\n",
    "        # block 3\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.act5 = nn.ReLU(True)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.classifer5 = nn.Sequential(\n",
    "                    nn.AvgPool2d(8),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(256, num_classes)\n",
    "                )\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.act6 = nn.ReLU(True)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.pool6 = nn.MaxPool2d(2, 2)\n",
    "        # classifier\n",
    "        self.classifer6 = nn.Sequential(\n",
    "            nn.AvgPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f1 = self.bn1(self.act1(self.conv1(x)))\n",
    "        c1 = self.classifer1(f1)\n",
    "        f2 = self.pool2(self.bn2(self.act2(self.conv2(f1))))\n",
    "        c2 = self.classifer2(f2)\n",
    "        f3 = self.bn3(self.act3(self.conv3(f2)))\n",
    "        c3 = self.classifer3(f3)\n",
    "        f4 = self.pool4(self.bn4(self.act4(self.conv4(f3))))\n",
    "        c4 = self.classifer4(f4)\n",
    "        f5 = self.bn5(self.act5(self.conv5(f4)))\n",
    "        c5 = self.classifer5(f5)\n",
    "        f6 = self.pool6(self.bn6(self.act6(self.conv6(f5))))\n",
    "        c6 = self.classifer6(f6)\n",
    "        return [c1,c2,c3,c4,c5,c6],[f1,f2,f3,f4,f5,f6]\n",
    "\n",
    "class STL_VGG7_64_plane(MTL_VGG7_64_plane_base):\n",
    "    def forward(self, x):\n",
    "        logits,features = super().forward(x)\n",
    "        return logits[-1]\n",
    "    \n",
    "# model = MTL_VGG7_64_plane_base()\n",
    "model = STL_VGG7_64_plane(num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f304402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0097,  0.2046,  0.2572,  0.0935, -0.2156, -0.0669, -0.3626, -0.0883,\n",
       "          0.8245,  0.5718,  0.2787,  0.1487, -0.0501, -0.5334,  0.4530,  0.3650,\n",
       "         -0.4797, -0.0073, -0.0293, -0.7292,  0.6386, -0.1352, -0.2711, -0.1982,\n",
       "         -0.6073, -0.0363, -0.1388,  0.3304, -0.5773, -0.4674, -0.5428,  0.3697,\n",
       "         -0.3469,  0.3962, -0.9787, -0.3460,  0.0211, -0.1735, -0.8395, -0.2057,\n",
       "         -0.3186, -0.1891, -0.5336,  0.5215, -0.0088, -0.4101,  0.9816, -0.2325,\n",
       "         -0.0344, -0.0623, -0.1496, -0.6476, -0.3163,  0.1586, -0.1181, -0.5792,\n",
       "          0.3304, -0.1607,  0.3075, -0.5142,  0.3612, -0.3138,  0.1902,  0.1615,\n",
       "          0.0218,  0.6882,  0.4684, -0.7952, -0.3055,  1.6140,  0.4651, -0.0494,\n",
       "         -0.3264, -0.8150, -0.2328, -0.1983,  0.1881, -0.0018,  0.2892,  0.0460,\n",
       "         -0.1453,  0.1170, -1.1655, -0.1644, -0.6019,  0.0601,  0.4371, -0.3548,\n",
       "          0.4765, -0.4159, -0.3010, -1.2608,  0.3197,  0.4112,  0.3718, -0.4092,\n",
       "          0.0233, -0.2345,  0.6654, -0.3500]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.zeros(1,3,32,32)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad671a21",
   "metadata": {},
   "source": [
    "# VGG on Cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd6639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet import ResNetForCifar10,BasicBlock,resnet18\n",
    "# model = ResNetForCifar10(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "# model = resnet18().cuda()\n",
    "# model=model.cuda()\n",
    "# model = sdn_stl_resnet18(num_classes=100)\n",
    "model = model.cuda()\n",
    "model_name = \"stl_vgg64_7_plane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06587ee7-3399-4e26-b7aa-52b7eb30f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Criterition\n",
    "lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 200\n",
    "batch_time,data_time,loss,metrics = np.zeros(epochs,),np.zeros(epochs,),np.zeros(epochs,),np.zeros(epochs,)\n",
    "val_loss,val_metrics=np.zeros(epochs,),np.zeros(epochs,)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optim,[75,130,180],gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f30af6-5ac4-404f-9267-e76e9cae9eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################epoch 1#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [0][100/100]\tTime 0.100 (0.141)\tData 0.066 (0.072)\tLoss 3.9185 (4.1778)\ttop1 9.600 (6.828)\n",
      "Test: [0/100]\tTime 0.431 (0.431)\tLoss 3.9312 (3.9312)\ttop1 16.000 (16.000)\n",
      " * top1 12.000\n",
      "#################epoch 2#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [1][100/100]\tTime 0.131 (0.118)\tData 0.098 (0.073)\tLoss 3.4512 (3.6939)\ttop1 16.600 (13.752)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 3.3516 (3.3516)\ttop1 21.000 (21.000)\n",
      " * top1 20.500\n",
      "#################epoch 3#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [2][100/100]\tTime 0.109 (0.118)\tData 0.077 (0.076)\tLoss 3.3204 (3.3522)\ttop1 18.200 (19.526)\n",
      "Test: [0/100]\tTime 0.386 (0.386)\tLoss 3.1553 (3.1553)\ttop1 21.000 (21.000)\n",
      " * top1 26.370\n",
      "#################epoch 4#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [3][100/100]\tTime 0.118 (0.118)\tData 0.084 (0.075)\tLoss 3.0219 (3.0775)\ttop1 26.600 (24.556)\n",
      "Test: [0/100]\tTime 0.401 (0.401)\tLoss 2.6166 (2.6166)\ttop1 34.000 (34.000)\n",
      " * top1 36.800\n",
      "#################epoch 5#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [4][100/100]\tTime 0.105 (0.117)\tData 0.074 (0.069)\tLoss 2.8064 (2.8556)\ttop1 30.000 (28.996)\n",
      "Test: [0/100]\tTime 0.402 (0.402)\tLoss 2.3369 (2.3369)\ttop1 48.000 (48.000)\n",
      " * top1 40.250\n",
      "#################epoch 6#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [5][100/100]\tTime 0.115 (0.119)\tData 0.080 (0.075)\tLoss 2.7089 (2.6630)\ttop1 31.200 (33.206)\n",
      "Test: [0/100]\tTime 0.408 (0.408)\tLoss 2.2543 (2.2543)\ttop1 41.000 (41.000)\n",
      " * top1 44.870\n",
      "#################epoch 7#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [6][100/100]\tTime 0.112 (0.118)\tData 0.078 (0.070)\tLoss 2.4723 (2.5343)\ttop1 37.200 (35.854)\n",
      "Test: [0/100]\tTime 0.395 (0.395)\tLoss 2.0937 (2.0937)\ttop1 47.000 (47.000)\n",
      " * top1 47.080\n",
      "#################epoch 8#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [7][100/100]\tTime 0.137 (0.120)\tData 0.101 (0.071)\tLoss 2.3289 (2.4059)\ttop1 39.600 (38.428)\n",
      "Test: [0/100]\tTime 0.394 (0.394)\tLoss 2.1186 (2.1186)\ttop1 47.000 (47.000)\n",
      " * top1 47.950\n",
      "#################epoch 9#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [8][100/100]\tTime 0.126 (0.118)\tData 0.091 (0.076)\tLoss 2.3362 (2.3232)\ttop1 41.800 (40.320)\n",
      "Test: [0/100]\tTime 0.443 (0.443)\tLoss 1.8172 (1.8172)\ttop1 48.000 (48.000)\n",
      " * top1 52.080\n",
      "#################epoch 10#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [9][100/100]\tTime 0.117 (0.119)\tData 0.078 (0.073)\tLoss 2.2810 (2.2328)\ttop1 40.800 (42.378)\n",
      "Test: [0/100]\tTime 0.407 (0.407)\tLoss 1.8537 (1.8537)\ttop1 49.000 (49.000)\n",
      " * top1 53.810\n",
      "#################epoch 11#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [10][100/100]\tTime 0.114 (0.119)\tData 0.080 (0.070)\tLoss 2.1826 (2.1608)\ttop1 43.400 (44.192)\n",
      "Test: [0/100]\tTime 0.406 (0.406)\tLoss 1.6602 (1.6602)\ttop1 58.000 (58.000)\n",
      " * top1 54.300\n",
      "#################epoch 12#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [11][100/100]\tTime 0.113 (0.122)\tData 0.076 (0.076)\tLoss 2.1069 (2.1119)\ttop1 44.800 (45.096)\n",
      "Test: [0/100]\tTime 0.400 (0.400)\tLoss 1.6493 (1.6493)\ttop1 58.000 (58.000)\n",
      " * top1 55.910\n",
      "#################epoch 13#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [12][100/100]\tTime 0.110 (0.125)\tData 0.075 (0.076)\tLoss 1.8968 (2.0482)\ttop1 51.600 (46.542)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.7177 (1.7177)\ttop1 55.000 (55.000)\n",
      " * top1 57.200\n",
      "#################epoch 14#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [13][100/100]\tTime 0.112 (0.118)\tData 0.077 (0.073)\tLoss 2.0246 (1.9983)\ttop1 46.400 (47.544)\n",
      "Test: [0/100]\tTime 0.396 (0.396)\tLoss 1.5583 (1.5583)\ttop1 62.000 (62.000)\n",
      " * top1 56.640\n",
      "#################epoch 15#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [14][100/100]\tTime 0.107 (0.119)\tData 0.073 (0.072)\tLoss 1.9232 (1.9586)\ttop1 49.800 (48.754)\n",
      "Test: [0/100]\tTime 0.414 (0.414)\tLoss 1.6935 (1.6935)\ttop1 58.000 (58.000)\n",
      " * top1 58.970\n",
      "#################epoch 16#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [15][100/100]\tTime 0.116 (0.123)\tData 0.082 (0.076)\tLoss 1.8621 (1.9363)\ttop1 50.800 (49.078)\n",
      "Test: [0/100]\tTime 0.414 (0.414)\tLoss 1.5115 (1.5115)\ttop1 55.000 (55.000)\n",
      " * top1 59.400\n",
      "#################epoch 17#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [16][100/100]\tTime 0.110 (0.120)\tData 0.077 (0.076)\tLoss 1.9510 (1.9006)\ttop1 49.000 (49.894)\n",
      "Test: [0/100]\tTime 0.401 (0.401)\tLoss 1.5258 (1.5258)\ttop1 65.000 (65.000)\n",
      " * top1 60.020\n",
      "#################epoch 18#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [17][100/100]\tTime 0.141 (0.121)\tData 0.108 (0.072)\tLoss 1.8873 (1.8683)\ttop1 51.000 (50.898)\n",
      "Test: [0/100]\tTime 0.417 (0.417)\tLoss 1.4127 (1.4127)\ttop1 66.000 (66.000)\n",
      " * top1 60.810\n",
      "#################epoch 19#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [18][100/100]\tTime 0.113 (0.119)\tData 0.078 (0.074)\tLoss 1.8548 (1.8439)\ttop1 50.800 (50.996)\n",
      "Test: [0/100]\tTime 0.407 (0.407)\tLoss 1.5573 (1.5573)\ttop1 60.000 (60.000)\n",
      " * top1 61.170\n",
      "#################epoch 20#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [19][100/100]\tTime 0.107 (0.124)\tData 0.074 (0.076)\tLoss 1.9846 (1.8190)\ttop1 46.600 (51.738)\n",
      "Test: [0/100]\tTime 0.410 (0.410)\tLoss 1.4432 (1.4432)\ttop1 60.000 (60.000)\n",
      " * top1 61.550\n",
      "#################epoch 21#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [20][100/100]\tTime 0.110 (0.121)\tData 0.080 (0.073)\tLoss 1.9484 (1.7974)\ttop1 47.800 (52.254)\n",
      "Test: [0/100]\tTime 0.415 (0.415)\tLoss 1.3362 (1.3362)\ttop1 62.000 (62.000)\n",
      " * top1 61.930\n",
      "#################epoch 22#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [21][100/100]\tTime 0.107 (0.116)\tData 0.073 (0.068)\tLoss 1.7431 (1.7541)\ttop1 53.200 (53.294)\n",
      "Test: [0/100]\tTime 0.415 (0.415)\tLoss 1.3171 (1.3171)\ttop1 67.000 (67.000)\n",
      " * top1 61.430\n",
      "#################epoch 23#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [22][100/100]\tTime 0.118 (0.119)\tData 0.085 (0.072)\tLoss 1.7250 (1.7455)\ttop1 55.600 (53.498)\n",
      "Test: [0/100]\tTime 0.410 (0.410)\tLoss 1.4581 (1.4581)\ttop1 62.000 (62.000)\n",
      " * top1 62.390\n",
      "#################epoch 24#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [23][100/100]\tTime 0.110 (0.117)\tData 0.077 (0.069)\tLoss 1.6982 (1.7262)\ttop1 52.000 (54.038)\n",
      "Test: [0/100]\tTime 0.415 (0.415)\tLoss 1.2514 (1.2514)\ttop1 64.000 (64.000)\n",
      " * top1 63.400\n",
      "#################epoch 25#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [24][100/100]\tTime 0.109 (0.114)\tData 0.076 (0.066)\tLoss 1.6203 (1.7035)\ttop1 59.000 (54.436)\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 1.3171 (1.3171)\ttop1 63.000 (63.000)\n",
      " * top1 61.940\n",
      "#################epoch 26#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [25][100/100]\tTime 0.110 (0.124)\tData 0.076 (0.077)\tLoss 1.6320 (1.6924)\ttop1 55.000 (54.634)\n",
      "Test: [0/100]\tTime 0.378 (0.378)\tLoss 1.3993 (1.3993)\ttop1 59.000 (59.000)\n",
      " * top1 62.920\n",
      "#################epoch 27#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [26][100/100]\tTime 0.105 (0.119)\tData 0.073 (0.071)\tLoss 1.6766 (1.6670)\ttop1 54.800 (55.258)\n",
      "Test: [0/100]\tTime 0.416 (0.416)\tLoss 1.2870 (1.2870)\ttop1 61.000 (61.000)\n",
      " * top1 63.610\n",
      "#################epoch 28#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [27][100/100]\tTime 0.110 (0.114)\tData 0.078 (0.070)\tLoss 1.6452 (1.6584)\ttop1 57.200 (55.452)\n",
      "Test: [0/100]\tTime 0.404 (0.404)\tLoss 1.4043 (1.4043)\ttop1 60.000 (60.000)\n",
      " * top1 63.250\n",
      "#################epoch 29#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [28][100/100]\tTime 0.114 (0.122)\tData 0.078 (0.076)\tLoss 1.7599 (1.6411)\ttop1 54.800 (56.138)\n",
      "Test: [0/100]\tTime 0.410 (0.410)\tLoss 1.4147 (1.4147)\ttop1 62.000 (62.000)\n",
      " * top1 64.510\n",
      "#################epoch 30#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [29][100/100]\tTime 0.108 (0.118)\tData 0.076 (0.072)\tLoss 1.6845 (1.6236)\ttop1 52.800 (56.388)\n",
      "Test: [0/100]\tTime 0.389 (0.389)\tLoss 1.3458 (1.3458)\ttop1 64.000 (64.000)\n",
      " * top1 63.020\n",
      "#################epoch 31#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [30][100/100]\tTime 0.107 (0.120)\tData 0.072 (0.071)\tLoss 1.5648 (1.5952)\ttop1 57.200 (56.910)\n",
      "Test: [0/100]\tTime 0.425 (0.425)\tLoss 1.4075 (1.4075)\ttop1 61.000 (61.000)\n",
      " * top1 64.800\n",
      "#################epoch 32#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [31][100/100]\tTime 0.113 (0.122)\tData 0.077 (0.073)\tLoss 1.4744 (1.5986)\ttop1 59.200 (56.984)\n",
      "Test: [0/100]\tTime 0.409 (0.409)\tLoss 1.3434 (1.3434)\ttop1 65.000 (65.000)\n",
      " * top1 63.640\n",
      "#################epoch 33#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [32][100/100]\tTime 0.111 (0.116)\tData 0.075 (0.066)\tLoss 1.5079 (1.5908)\ttop1 58.200 (57.112)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.3731 (1.3731)\ttop1 64.000 (64.000)\n",
      " * top1 65.330\n",
      "#################epoch 34#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [33][100/100]\tTime 0.112 (0.117)\tData 0.079 (0.071)\tLoss 1.5632 (1.5782)\ttop1 56.000 (57.238)\n",
      "Test: [0/100]\tTime 0.391 (0.391)\tLoss 1.2479 (1.2479)\ttop1 64.000 (64.000)\n",
      " * top1 65.260\n",
      "#################epoch 35#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [34][100/100]\tTime 0.107 (0.118)\tData 0.075 (0.076)\tLoss 1.5059 (1.5563)\ttop1 58.200 (58.292)\n",
      "Test: [0/100]\tTime 0.413 (0.413)\tLoss 1.0661 (1.0661)\ttop1 72.000 (72.000)\n",
      " * top1 64.380\n",
      "#################epoch 36#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [35][100/100]\tTime 0.107 (0.117)\tData 0.075 (0.069)\tLoss 1.5622 (1.5525)\ttop1 57.800 (58.056)\n",
      "Test: [0/100]\tTime 0.417 (0.417)\tLoss 1.5452 (1.5452)\ttop1 62.000 (62.000)\n",
      " * top1 63.770\n",
      "#################epoch 37#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [36][100/100]\tTime 0.120 (0.123)\tData 0.087 (0.074)\tLoss 1.5476 (1.5371)\ttop1 56.200 (58.350)\n",
      "Test: [0/100]\tTime 0.415 (0.415)\tLoss 1.2606 (1.2606)\ttop1 69.000 (69.000)\n",
      " * top1 66.310\n",
      "#################epoch 38#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [37][100/100]\tTime 0.112 (0.122)\tData 0.080 (0.076)\tLoss 1.4138 (1.5237)\ttop1 60.600 (58.850)\n",
      "Test: [0/100]\tTime 0.434 (0.434)\tLoss 1.3155 (1.3155)\ttop1 63.000 (63.000)\n",
      " * top1 65.130\n",
      "#################epoch 39#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [38][100/100]\tTime 0.107 (0.115)\tData 0.073 (0.069)\tLoss 1.6152 (1.5289)\ttop1 56.000 (58.694)\n",
      "Test: [0/100]\tTime 0.423 (0.423)\tLoss 1.3632 (1.3632)\ttop1 68.000 (68.000)\n",
      " * top1 64.770\n",
      "#################epoch 40#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [39][100/100]\tTime 0.110 (0.116)\tData 0.078 (0.072)\tLoss 1.6200 (1.5014)\ttop1 59.200 (59.258)\n",
      "Test: [0/100]\tTime 0.406 (0.406)\tLoss 1.1889 (1.1889)\ttop1 70.000 (70.000)\n",
      " * top1 65.340\n",
      "#################epoch 41#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [40][100/100]\tTime 0.109 (0.118)\tData 0.074 (0.069)\tLoss 1.4239 (1.5007)\ttop1 59.600 (59.330)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.2599 (1.2599)\ttop1 65.000 (65.000)\n",
      " * top1 65.790\n",
      "#################epoch 42#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [41][100/100]\tTime 0.111 (0.116)\tData 0.077 (0.071)\tLoss 1.5242 (1.5050)\ttop1 57.400 (59.122)\n",
      "Test: [0/100]\tTime 0.401 (0.401)\tLoss 1.1296 (1.1296)\ttop1 66.000 (66.000)\n",
      " * top1 65.580\n",
      "#################epoch 43#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [42][100/100]\tTime 0.108 (0.121)\tData 0.076 (0.078)\tLoss 1.5423 (1.4875)\ttop1 56.400 (59.694)\n",
      "Test: [0/100]\tTime 0.422 (0.422)\tLoss 1.1050 (1.1050)\ttop1 71.000 (71.000)\n",
      " * top1 67.030\n",
      "#################epoch 44#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [43][100/100]\tTime 0.111 (0.119)\tData 0.076 (0.070)\tLoss 1.5460 (1.4805)\ttop1 59.400 (59.982)\n",
      "Test: [0/100]\tTime 0.410 (0.410)\tLoss 1.4302 (1.4302)\ttop1 64.000 (64.000)\n",
      " * top1 65.640\n",
      "#################epoch 45#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [44][100/100]\tTime 0.110 (0.122)\tData 0.075 (0.075)\tLoss 1.4505 (1.4754)\ttop1 61.000 (59.994)\n",
      "Test: [0/100]\tTime 0.398 (0.398)\tLoss 1.3993 (1.3993)\ttop1 65.000 (65.000)\n",
      " * top1 66.840\n",
      "#################epoch 46#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [45][100/100]\tTime 0.107 (0.121)\tData 0.072 (0.074)\tLoss 1.4677 (1.4544)\ttop1 61.200 (60.518)\n",
      "Test: [0/100]\tTime 0.389 (0.389)\tLoss 1.3208 (1.3208)\ttop1 67.000 (67.000)\n",
      " * top1 66.780\n",
      "#################epoch 47#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [46][100/100]\tTime 0.106 (0.116)\tData 0.074 (0.069)\tLoss 1.3620 (1.4478)\ttop1 64.600 (60.730)\n",
      "Test: [0/100]\tTime 0.407 (0.407)\tLoss 1.2332 (1.2332)\ttop1 64.000 (64.000)\n",
      " * top1 66.920\n",
      "#################epoch 48#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [47][100/100]\tTime 0.108 (0.124)\tData 0.074 (0.074)\tLoss 1.3791 (1.4493)\ttop1 61.600 (60.590)\n",
      "Test: [0/100]\tTime 0.394 (0.394)\tLoss 1.2808 (1.2808)\ttop1 66.000 (66.000)\n",
      " * top1 66.370\n",
      "#################epoch 49#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [48][100/100]\tTime 0.113 (0.120)\tData 0.083 (0.073)\tLoss 1.4808 (1.4423)\ttop1 61.000 (60.822)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.1541 (1.1541)\ttop1 72.000 (72.000)\n",
      " * top1 66.290\n",
      "#################epoch 50#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [49][100/100]\tTime 0.109 (0.117)\tData 0.074 (0.072)\tLoss 1.4694 (1.4377)\ttop1 60.200 (60.906)\n",
      "Test: [0/100]\tTime 0.420 (0.420)\tLoss 1.0297 (1.0297)\ttop1 69.000 (69.000)\n",
      " * top1 66.720\n",
      "#################epoch 51#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [50][100/100]\tTime 0.110 (0.115)\tData 0.075 (0.071)\tLoss 1.4110 (1.4315)\ttop1 61.200 (61.046)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.4258 (1.4258)\ttop1 62.000 (62.000)\n",
      " * top1 65.770\n",
      "#################epoch 52#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [51][100/100]\tTime 0.109 (0.116)\tData 0.076 (0.069)\tLoss 1.4024 (1.4171)\ttop1 61.800 (61.328)\n",
      "Test: [0/100]\tTime 0.405 (0.405)\tLoss 1.1889 (1.1889)\ttop1 67.000 (67.000)\n",
      " * top1 66.700\n",
      "#################epoch 53#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [52][100/100]\tTime 0.111 (0.119)\tData 0.076 (0.075)\tLoss 1.4855 (1.4126)\ttop1 61.400 (61.420)\n",
      "Test: [0/100]\tTime 0.409 (0.409)\tLoss 1.4474 (1.4474)\ttop1 65.000 (65.000)\n",
      " * top1 67.310\n",
      "#################epoch 54#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [53][100/100]\tTime 0.110 (0.116)\tData 0.074 (0.068)\tLoss 1.4839 (1.4126)\ttop1 61.400 (61.478)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.2247 (1.2247)\ttop1 65.000 (65.000)\n",
      " * top1 66.690\n",
      "#################epoch 55#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [54][100/100]\tTime 0.115 (0.117)\tData 0.078 (0.078)\tLoss 1.4448 (1.4184)\ttop1 61.600 (61.386)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.2791 (1.2791)\ttop1 64.000 (64.000)\n",
      " * top1 66.750\n",
      "#################epoch 56#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [55][100/100]\tTime 0.102 (0.118)\tData 0.070 (0.075)\tLoss 1.4765 (1.4072)\ttop1 59.600 (61.872)\n",
      "Test: [0/100]\tTime 0.411 (0.411)\tLoss 1.2961 (1.2961)\ttop1 69.000 (69.000)\n",
      " * top1 67.320\n",
      "#################epoch 57#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [56][100/100]\tTime 0.104 (0.115)\tData 0.072 (0.066)\tLoss 1.4544 (1.3944)\ttop1 60.400 (61.836)\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 1.2867 (1.2867)\ttop1 65.000 (65.000)\n",
      " * top1 67.620\n",
      "#################epoch 58#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [57][100/100]\tTime 0.102 (0.120)\tData 0.071 (0.075)\tLoss 1.3998 (1.3976)\ttop1 62.000 (61.668)\n",
      "Test: [0/100]\tTime 0.392 (0.392)\tLoss 1.2162 (1.2162)\ttop1 65.000 (65.000)\n",
      " * top1 66.270\n",
      "#################epoch 59#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [58][100/100]\tTime 0.109 (0.116)\tData 0.076 (0.068)\tLoss 1.5248 (1.3825)\ttop1 57.600 (62.150)\n",
      "Test: [0/100]\tTime 0.394 (0.394)\tLoss 1.2793 (1.2793)\ttop1 69.000 (69.000)\n",
      " * top1 66.760\n",
      "#################epoch 60#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [59][100/100]\tTime 0.110 (0.117)\tData 0.078 (0.072)\tLoss 1.4030 (1.3904)\ttop1 63.000 (61.990)\n",
      "Test: [0/100]\tTime 0.412 (0.412)\tLoss 1.3096 (1.3096)\ttop1 64.000 (64.000)\n",
      " * top1 67.050\n",
      "#################epoch 61#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [60][100/100]\tTime 0.107 (0.123)\tData 0.075 (0.074)\tLoss 1.3547 (1.3733)\ttop1 62.400 (62.560)\n",
      "Test: [0/100]\tTime 0.402 (0.402)\tLoss 1.2934 (1.2934)\ttop1 69.000 (69.000)\n",
      " * top1 65.640\n",
      "#################epoch 62#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [61][100/100]\tTime 0.114 (0.122)\tData 0.081 (0.073)\tLoss 1.3165 (1.3604)\ttop1 63.600 (62.904)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.2842 (1.2842)\ttop1 71.000 (71.000)\n",
      " * top1 67.880\n",
      "#################epoch 63#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [62][100/100]\tTime 0.110 (0.117)\tData 0.074 (0.071)\tLoss 1.5034 (1.3512)\ttop1 59.000 (62.910)\n",
      "Test: [0/100]\tTime 0.388 (0.388)\tLoss 1.1498 (1.1498)\ttop1 70.000 (70.000)\n",
      " * top1 67.850\n",
      "#################epoch 64#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [63][100/100]\tTime 0.110 (0.116)\tData 0.077 (0.070)\tLoss 1.4622 (1.3711)\ttop1 59.000 (62.842)\n",
      "Test: [0/100]\tTime 0.408 (0.408)\tLoss 1.2852 (1.2852)\ttop1 69.000 (69.000)\n",
      " * top1 67.220\n",
      "#################epoch 65#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [64][100/100]\tTime 0.107 (0.123)\tData 0.075 (0.074)\tLoss 1.3016 (1.3494)\ttop1 64.200 (63.166)\n",
      "Test: [0/100]\tTime 0.410 (0.410)\tLoss 1.2924 (1.2924)\ttop1 69.000 (69.000)\n",
      " * top1 67.760\n",
      "#################epoch 66#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [65][100/100]\tTime 0.109 (0.115)\tData 0.076 (0.072)\tLoss 1.3361 (1.3610)\ttop1 63.800 (62.778)\n",
      "Test: [0/100]\tTime 0.395 (0.395)\tLoss 1.1335 (1.1335)\ttop1 73.000 (73.000)\n",
      " * top1 66.660\n",
      "#################epoch 67#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [66][100/100]\tTime 0.107 (0.117)\tData 0.074 (0.070)\tLoss 1.4711 (1.3542)\ttop1 60.000 (63.166)\n",
      "Test: [0/100]\tTime 0.409 (0.409)\tLoss 1.2734 (1.2734)\ttop1 73.000 (73.000)\n",
      " * top1 67.690\n",
      "#################epoch 68#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [67][100/100]\tTime 0.104 (0.115)\tData 0.072 (0.070)\tLoss 1.2854 (1.3267)\ttop1 64.200 (63.596)\n",
      "Test: [0/100]\tTime 0.404 (0.404)\tLoss 1.2610 (1.2610)\ttop1 72.000 (72.000)\n",
      " * top1 66.740\n",
      "#################epoch 69#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [68][100/100]\tTime 0.107 (0.121)\tData 0.073 (0.074)\tLoss 1.2980 (1.3369)\ttop1 63.600 (63.204)\n",
      "Test: [0/100]\tTime 0.406 (0.406)\tLoss 1.3025 (1.3025)\ttop1 70.000 (70.000)\n",
      " * top1 67.020\n",
      "#################epoch 70#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [69][100/100]\tTime 0.110 (0.115)\tData 0.077 (0.068)\tLoss 1.3141 (1.3349)\ttop1 61.200 (63.432)\n",
      "Test: [0/100]\tTime 0.418 (0.418)\tLoss 1.3603 (1.3603)\ttop1 70.000 (70.000)\n",
      " * top1 67.830\n",
      "#################epoch 71#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [70][100/100]\tTime 0.105 (0.120)\tData 0.073 (0.073)\tLoss 1.3968 (1.3316)\ttop1 60.800 (63.402)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.5296 (1.5296)\ttop1 64.000 (64.000)\n",
      " * top1 66.970\n",
      "#################epoch 72#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [71][100/100]\tTime 0.110 (0.116)\tData 0.075 (0.076)\tLoss 1.2880 (1.3212)\ttop1 62.200 (63.816)\n",
      "Test: [0/100]\tTime 0.417 (0.417)\tLoss 1.1881 (1.1881)\ttop1 74.000 (74.000)\n",
      " * top1 68.370\n",
      "#################epoch 73#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [72][100/100]\tTime 0.108 (0.123)\tData 0.076 (0.076)\tLoss 1.3509 (1.3281)\ttop1 62.800 (63.868)\n",
      "Test: [0/100]\tTime 0.415 (0.415)\tLoss 1.3037 (1.3037)\ttop1 70.000 (70.000)\n",
      " * top1 68.560\n",
      "#################epoch 74#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [73][100/100]\tTime 0.108 (0.122)\tData 0.073 (0.073)\tLoss 1.3493 (1.3126)\ttop1 62.800 (63.960)\n",
      "Test: [0/100]\tTime 0.401 (0.401)\tLoss 1.2968 (1.2968)\ttop1 71.000 (71.000)\n",
      " * top1 66.930\n",
      "#################epoch 75#################\n",
      "lr=0.100000 \n",
      "\n",
      "Epoch: [74][100/100]\tTime 0.109 (0.115)\tData 0.072 (0.066)\tLoss 1.3726 (1.3182)\ttop1 62.200 (63.510)\n",
      "Test: [0/100]\tTime 0.386 (0.386)\tLoss 1.2136 (1.2136)\ttop1 75.000 (75.000)\n",
      " * top1 67.880\n",
      "#################epoch 76#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [75][100/100]\tTime 0.109 (0.116)\tData 0.077 (0.071)\tLoss 1.1066 (1.2015)\ttop1 70.200 (66.976)\n",
      "Test: [0/100]\tTime 0.437 (0.437)\tLoss 1.1671 (1.1671)\ttop1 74.000 (74.000)\n",
      " * top1 70.940\n",
      "#################epoch 77#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [76][100/100]\tTime 0.113 (0.118)\tData 0.078 (0.072)\tLoss 1.1073 (1.1608)\ttop1 68.200 (67.988)\n",
      "Test: [0/100]\tTime 0.414 (0.414)\tLoss 1.1590 (1.1590)\ttop1 72.000 (72.000)\n",
      " * top1 71.370\n",
      "#################epoch 78#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [77][100/100]\tTime 0.107 (0.115)\tData 0.077 (0.071)\tLoss 1.1497 (1.1286)\ttop1 66.800 (68.706)\n",
      "Test: [0/100]\tTime 0.412 (0.412)\tLoss 1.1858 (1.1858)\ttop1 73.000 (73.000)\n",
      " * top1 71.840\n",
      "#################epoch 79#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [78][100/100]\tTime 0.107 (0.122)\tData 0.075 (0.075)\tLoss 1.0955 (1.1211)\ttop1 70.800 (69.090)\n",
      "Test: [0/100]\tTime 0.394 (0.394)\tLoss 1.1628 (1.1628)\ttop1 73.000 (73.000)\n",
      " * top1 71.850\n",
      "#################epoch 80#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [79][100/100]\tTime 0.109 (0.115)\tData 0.075 (0.071)\tLoss 1.0590 (1.1199)\ttop1 70.600 (69.196)\n",
      "Test: [0/100]\tTime 0.419 (0.419)\tLoss 1.1038 (1.1038)\ttop1 72.000 (72.000)\n",
      " * top1 71.670\n",
      "#################epoch 81#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [80][100/100]\tTime 0.108 (0.120)\tData 0.075 (0.074)\tLoss 1.0599 (1.1024)\ttop1 70.400 (69.566)\n",
      "Test: [0/100]\tTime 0.391 (0.391)\tLoss 1.1560 (1.1560)\ttop1 72.000 (72.000)\n",
      " * top1 71.940\n",
      "#################epoch 82#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [81][100/100]\tTime 0.104 (0.115)\tData 0.075 (0.067)\tLoss 1.0962 (1.0843)\ttop1 68.200 (69.998)\n",
      "Test: [0/100]\tTime 0.405 (0.405)\tLoss 1.1562 (1.1562)\ttop1 72.000 (72.000)\n",
      " * top1 71.950\n",
      "#################epoch 83#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [82][100/100]\tTime 0.111 (0.115)\tData 0.076 (0.066)\tLoss 1.1112 (1.0907)\ttop1 71.200 (70.070)\n",
      "Test: [0/100]\tTime 0.398 (0.398)\tLoss 1.1592 (1.1592)\ttop1 73.000 (73.000)\n",
      " * top1 71.910\n",
      "#################epoch 84#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [83][100/100]\tTime 0.115 (0.116)\tData 0.080 (0.071)\tLoss 1.0813 (1.0951)\ttop1 72.000 (69.878)\n",
      "Test: [0/100]\tTime 0.387 (0.387)\tLoss 1.1653 (1.1653)\ttop1 73.000 (73.000)\n",
      " * top1 72.230\n",
      "#################epoch 85#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [84][100/100]\tTime 0.104 (0.116)\tData 0.075 (0.068)\tLoss 1.1160 (1.0746)\ttop1 68.200 (70.396)\n",
      "Test: [0/100]\tTime 0.408 (0.408)\tLoss 1.1521 (1.1521)\ttop1 72.000 (72.000)\n",
      " * top1 72.060\n",
      "#################epoch 86#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [85][100/100]\tTime 0.111 (0.122)\tData 0.079 (0.078)\tLoss 1.0456 (1.0784)\ttop1 73.200 (70.030)\n",
      "Test: [0/100]\tTime 0.409 (0.409)\tLoss 1.1299 (1.1299)\ttop1 74.000 (74.000)\n",
      " * top1 72.140\n",
      "#################epoch 87#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [86][100/100]\tTime 0.111 (0.124)\tData 0.077 (0.074)\tLoss 1.1495 (1.0778)\ttop1 68.400 (70.168)\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 1.1068 (1.1068)\ttop1 73.000 (73.000)\n",
      " * top1 72.230\n",
      "#################epoch 88#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [87][100/100]\tTime 0.116 (0.117)\tData 0.080 (0.071)\tLoss 1.0212 (1.0610)\ttop1 71.600 (70.732)\n",
      "Test: [0/100]\tTime 0.394 (0.394)\tLoss 1.1538 (1.1538)\ttop1 73.000 (73.000)\n",
      " * top1 72.050\n",
      "#################epoch 89#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [88][100/100]\tTime 0.109 (0.120)\tData 0.074 (0.073)\tLoss 1.1351 (1.0685)\ttop1 69.600 (70.636)\n",
      "Test: [0/100]\tTime 0.423 (0.423)\tLoss 1.1645 (1.1645)\ttop1 73.000 (73.000)\n",
      " * top1 71.960\n",
      "#################epoch 90#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [89][100/100]\tTime 0.111 (0.123)\tData 0.076 (0.075)\tLoss 1.1011 (1.0483)\ttop1 69.000 (70.940)\n",
      "Test: [0/100]\tTime 0.411 (0.411)\tLoss 1.1431 (1.1431)\ttop1 72.000 (72.000)\n",
      " * top1 71.770\n",
      "#################epoch 91#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [90][100/100]\tTime 0.108 (0.116)\tData 0.074 (0.067)\tLoss 1.0421 (1.0541)\ttop1 72.000 (70.754)\n",
      "Test: [0/100]\tTime 0.420 (0.420)\tLoss 1.1517 (1.1517)\ttop1 74.000 (74.000)\n",
      " * top1 72.210\n",
      "#################epoch 92#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [91][100/100]\tTime 0.103 (0.114)\tData 0.073 (0.066)\tLoss 0.9841 (1.0436)\ttop1 72.600 (71.186)\n",
      "Test: [0/100]\tTime 0.388 (0.388)\tLoss 1.1403 (1.1403)\ttop1 72.000 (72.000)\n",
      " * top1 72.680\n",
      "#################epoch 93#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [92][100/100]\tTime 0.112 (0.121)\tData 0.076 (0.075)\tLoss 1.1231 (1.0533)\ttop1 69.200 (70.798)\n",
      "Test: [0/100]\tTime 0.407 (0.407)\tLoss 1.1697 (1.1697)\ttop1 72.000 (72.000)\n",
      " * top1 72.440\n",
      "#################epoch 94#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [93][100/100]\tTime 0.107 (0.123)\tData 0.073 (0.073)\tLoss 1.0853 (1.0473)\ttop1 68.400 (71.220)\n",
      "Test: [0/100]\tTime 0.388 (0.388)\tLoss 1.1488 (1.1488)\ttop1 73.000 (73.000)\n",
      " * top1 72.410\n",
      "#################epoch 95#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [94][100/100]\tTime 0.106 (0.121)\tData 0.072 (0.074)\tLoss 1.0381 (1.0378)\ttop1 71.000 (71.148)\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 1.1581 (1.1581)\ttop1 73.000 (73.000)\n",
      " * top1 72.390\n",
      "#################epoch 96#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [95][100/100]\tTime 0.109 (0.116)\tData 0.072 (0.067)\tLoss 1.0595 (1.0378)\ttop1 71.400 (71.272)\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 1.1672 (1.1672)\ttop1 72.000 (72.000)\n",
      " * top1 72.500\n",
      "#################epoch 97#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [96][100/100]\tTime 0.112 (0.120)\tData 0.078 (0.077)\tLoss 1.1042 (1.0358)\ttop1 68.800 (71.344)\n",
      "Test: [0/100]\tTime 0.417 (0.417)\tLoss 1.1463 (1.1463)\ttop1 73.000 (73.000)\n",
      " * top1 72.340\n",
      "#################epoch 98#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [97][100/100]\tTime 0.104 (0.119)\tData 0.075 (0.073)\tLoss 1.2091 (1.0358)\ttop1 66.800 (71.298)\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 1.1723 (1.1723)\ttop1 74.000 (74.000)\n",
      " * top1 72.340\n",
      "#################epoch 99#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [98][100/100]\tTime 0.106 (0.118)\tData 0.074 (0.073)\tLoss 0.9908 (1.0345)\ttop1 73.400 (71.364)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.1905 (1.1905)\ttop1 72.000 (72.000)\n",
      " * top1 72.460\n",
      "#################epoch 100#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [99][100/100]\tTime 0.107 (0.116)\tData 0.075 (0.069)\tLoss 1.1232 (1.0267)\ttop1 70.600 (71.500)\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 1.1911 (1.1911)\ttop1 72.000 (72.000)\n",
      " * top1 72.470\n",
      "#################epoch 101#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [100][100/100]\tTime 0.108 (0.115)\tData 0.072 (0.070)\tLoss 1.0704 (1.0207)\ttop1 69.200 (71.628)\n",
      "Test: [0/100]\tTime 0.382 (0.382)\tLoss 1.1533 (1.1533)\ttop1 74.000 (74.000)\n",
      " * top1 72.310\n",
      "#################epoch 102#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [101][100/100]\tTime 0.122 (0.116)\tData 0.089 (0.071)\tLoss 1.0954 (1.0235)\ttop1 71.200 (71.508)\n",
      "Test: [0/100]\tTime 0.387 (0.387)\tLoss 1.1905 (1.1905)\ttop1 74.000 (74.000)\n",
      " * top1 72.430\n",
      "#################epoch 103#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [102][100/100]\tTime 0.111 (0.123)\tData 0.073 (0.073)\tLoss 1.1109 (1.0196)\ttop1 69.200 (71.496)\n",
      "Test: [0/100]\tTime 0.393 (0.393)\tLoss 1.1274 (1.1274)\ttop1 75.000 (75.000)\n",
      " * top1 72.380\n",
      "#################epoch 104#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [103][100/100]\tTime 0.110 (0.119)\tData 0.075 (0.069)\tLoss 1.1500 (1.0136)\ttop1 70.200 (71.768)\n",
      "Test: [0/100]\tTime 0.402 (0.402)\tLoss 1.1369 (1.1369)\ttop1 73.000 (73.000)\n",
      " * top1 72.490\n",
      "#################epoch 105#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [104][100/100]\tTime 0.109 (0.116)\tData 0.073 (0.068)\tLoss 0.9989 (1.0153)\ttop1 72.000 (71.698)\n",
      "Test: [0/100]\tTime 0.383 (0.383)\tLoss 1.1324 (1.1324)\ttop1 75.000 (75.000)\n",
      " * top1 72.240\n",
      "#################epoch 106#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [105][100/100]\tTime 0.107 (0.122)\tData 0.072 (0.074)\tLoss 1.0688 (1.0074)\ttop1 70.800 (72.100)\n",
      "Test: [0/100]\tTime 0.389 (0.389)\tLoss 1.1814 (1.1814)\ttop1 75.000 (75.000)\n",
      " * top1 72.360\n",
      "#################epoch 107#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [106][100/100]\tTime 0.109 (0.116)\tData 0.073 (0.075)\tLoss 1.0433 (1.0133)\ttop1 70.800 (71.812)\n",
      "Test: [0/100]\tTime 0.395 (0.395)\tLoss 1.1591 (1.1591)\ttop1 73.000 (73.000)\n",
      " * top1 72.250\n",
      "#################epoch 108#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [107][100/100]\tTime 0.102 (0.116)\tData 0.070 (0.071)\tLoss 1.1058 (1.0029)\ttop1 69.600 (72.222)\n",
      "Test: [0/100]\tTime 0.379 (0.379)\tLoss 1.2043 (1.2043)\ttop1 74.000 (74.000)\n",
      " * top1 72.660\n",
      "#################epoch 109#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [108][100/100]\tTime 0.103 (0.116)\tData 0.072 (0.072)\tLoss 0.9669 (0.9982)\ttop1 73.400 (72.556)\n",
      "Test: [0/100]\tTime 0.392 (0.392)\tLoss 1.2231 (1.2231)\ttop1 73.000 (73.000)\n",
      " * top1 72.500\n",
      "#################epoch 110#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [109][100/100]\tTime 0.103 (0.116)\tData 0.069 (0.068)\tLoss 0.9741 (0.9989)\ttop1 74.600 (72.158)\n",
      "Test: [0/100]\tTime 0.413 (0.413)\tLoss 1.1982 (1.1982)\ttop1 74.000 (74.000)\n",
      " * top1 72.370\n",
      "#################epoch 111#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [110][100/100]\tTime 0.109 (0.121)\tData 0.073 (0.075)\tLoss 0.9894 (0.9998)\ttop1 73.000 (72.358)\n",
      "Test: [0/100]\tTime 0.389 (0.389)\tLoss 1.2096 (1.2096)\ttop1 72.000 (72.000)\n",
      " * top1 72.570\n",
      "#################epoch 112#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [111][100/100]\tTime 0.101 (0.115)\tData 0.068 (0.066)\tLoss 1.0138 (1.0018)\ttop1 71.200 (72.374)\n",
      "Test: [0/100]\tTime 0.383 (0.383)\tLoss 1.2184 (1.2184)\ttop1 72.000 (72.000)\n",
      " * top1 72.250\n",
      "#################epoch 113#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [112][100/100]\tTime 0.109 (0.120)\tData 0.072 (0.075)\tLoss 1.0469 (1.0044)\ttop1 70.400 (72.008)\n",
      "Test: [0/100]\tTime 0.400 (0.400)\tLoss 1.2195 (1.2195)\ttop1 75.000 (75.000)\n",
      " * top1 72.450\n",
      "#################epoch 114#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [113][100/100]\tTime 0.101 (0.117)\tData 0.067 (0.070)\tLoss 0.9005 (0.9973)\ttop1 74.600 (72.294)\n",
      "Test: [0/100]\tTime 0.393 (0.393)\tLoss 1.2120 (1.2120)\ttop1 74.000 (74.000)\n",
      " * top1 72.370\n",
      "#################epoch 115#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [114][100/100]\tTime 0.103 (0.123)\tData 0.069 (0.076)\tLoss 1.0199 (1.0013)\ttop1 71.600 (72.042)\n",
      "Test: [0/100]\tTime 0.385 (0.385)\tLoss 1.2243 (1.2243)\ttop1 73.000 (73.000)\n",
      " * top1 72.630\n",
      "#################epoch 116#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [115][100/100]\tTime 0.108 (0.117)\tData 0.073 (0.068)\tLoss 1.0932 (0.9989)\ttop1 71.400 (72.374)\n",
      "Test: [0/100]\tTime 0.373 (0.373)\tLoss 1.2290 (1.2290)\ttop1 73.000 (73.000)\n",
      " * top1 72.440\n",
      "#################epoch 117#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [116][100/100]\tTime 0.107 (0.115)\tData 0.075 (0.071)\tLoss 1.0642 (0.9905)\ttop1 70.800 (72.356)\n",
      "Test: [0/100]\tTime 0.370 (0.370)\tLoss 1.2016 (1.2016)\ttop1 73.000 (73.000)\n",
      " * top1 72.670\n",
      "#################epoch 118#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [117][100/100]\tTime 0.110 (0.117)\tData 0.076 (0.073)\tLoss 1.0220 (0.9829)\ttop1 70.000 (72.524)\n",
      "Test: [0/100]\tTime 0.381 (0.381)\tLoss 1.1897 (1.1897)\ttop1 73.000 (73.000)\n",
      " * top1 72.640\n",
      "#################epoch 119#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [118][100/100]\tTime 0.101 (0.117)\tData 0.069 (0.075)\tLoss 1.0345 (0.9861)\ttop1 69.200 (72.458)\n",
      "Test: [0/100]\tTime 0.396 (0.396)\tLoss 1.2038 (1.2038)\ttop1 74.000 (74.000)\n",
      " * top1 72.560\n",
      "#################epoch 120#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [119][100/100]\tTime 0.105 (0.119)\tData 0.072 (0.072)\tLoss 1.0575 (0.9921)\ttop1 70.200 (72.420)\n",
      "Test: [0/100]\tTime 0.373 (0.373)\tLoss 1.2106 (1.2106)\ttop1 75.000 (75.000)\n",
      " * top1 72.800\n",
      "#################epoch 121#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [120][100/100]\tTime 0.105 (0.119)\tData 0.071 (0.075)\tLoss 1.0322 (0.9867)\ttop1 71.800 (72.458)\n",
      "Test: [0/100]\tTime 0.387 (0.387)\tLoss 1.1923 (1.1923)\ttop1 73.000 (73.000)\n",
      " * top1 72.420\n",
      "#################epoch 122#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [121][100/100]\tTime 0.108 (0.117)\tData 0.075 (0.067)\tLoss 1.0597 (0.9800)\ttop1 69.600 (72.770)\n",
      "Test: [0/100]\tTime 0.414 (0.414)\tLoss 1.1942 (1.1942)\ttop1 71.000 (71.000)\n",
      " * top1 72.650\n",
      "#################epoch 123#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [122][100/100]\tTime 0.112 (0.121)\tData 0.076 (0.074)\tLoss 0.9867 (0.9947)\ttop1 72.600 (72.266)\n",
      "Test: [0/100]\tTime 0.374 (0.374)\tLoss 1.2100 (1.2100)\ttop1 72.000 (72.000)\n",
      " * top1 72.440\n",
      "#################epoch 124#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [123][100/100]\tTime 0.105 (0.115)\tData 0.071 (0.078)\tLoss 0.9981 (0.9758)\ttop1 72.800 (72.876)\n",
      "Test: [0/100]\tTime 0.371 (0.371)\tLoss 1.2232 (1.2232)\ttop1 73.000 (73.000)\n",
      " * top1 72.630\n",
      "#################epoch 125#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [124][100/100]\tTime 0.104 (0.114)\tData 0.070 (0.066)\tLoss 1.0126 (0.9759)\ttop1 73.000 (72.860)\n",
      "Test: [0/100]\tTime 0.395 (0.395)\tLoss 1.2023 (1.2023)\ttop1 72.000 (72.000)\n",
      " * top1 72.710\n",
      "#################epoch 126#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [125][100/100]\tTime 0.105 (0.115)\tData 0.070 (0.067)\tLoss 0.9014 (0.9898)\ttop1 72.200 (72.242)\n",
      "Test: [0/100]\tTime 0.397 (0.397)\tLoss 1.2056 (1.2056)\ttop1 73.000 (73.000)\n",
      " * top1 72.330\n",
      "#################epoch 127#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [126][100/100]\tTime 0.107 (0.120)\tData 0.075 (0.074)\tLoss 0.9502 (0.9790)\ttop1 71.800 (72.840)\n",
      "Test: [0/100]\tTime 0.371 (0.371)\tLoss 1.2283 (1.2283)\ttop1 72.000 (72.000)\n",
      " * top1 72.540\n",
      "#################epoch 128#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [127][100/100]\tTime 0.110 (0.116)\tData 0.074 (0.067)\tLoss 1.0233 (0.9749)\ttop1 72.400 (72.764)\n",
      "Test: [0/100]\tTime 0.378 (0.378)\tLoss 1.2047 (1.2047)\ttop1 75.000 (75.000)\n",
      " * top1 72.600\n",
      "#################epoch 129#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [128][100/100]\tTime 0.105 (0.116)\tData 0.074 (0.067)\tLoss 0.9606 (0.9744)\ttop1 73.000 (72.980)\n",
      "Test: [0/100]\tTime 0.372 (0.372)\tLoss 1.1921 (1.1921)\ttop1 75.000 (75.000)\n",
      " * top1 72.430\n",
      "#################epoch 130#################\n",
      "lr=0.010000 \n",
      "\n",
      "Epoch: [129][100/100]\tTime 0.105 (0.116)\tData 0.071 (0.067)\tLoss 1.0303 (0.9732)\ttop1 71.400 (72.846)\n",
      "Test: [0/100]\tTime 0.403 (0.403)\tLoss 1.2367 (1.2367)\ttop1 73.000 (73.000)\n",
      " * top1 72.360\n",
      "#################epoch 131#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [130][100/100]\tTime 0.106 (0.120)\tData 0.072 (0.073)\tLoss 0.9132 (0.9597)\ttop1 73.000 (73.252)\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 1.2264 (1.2264)\ttop1 73.000 (73.000)\n",
      " * top1 72.720\n",
      "#################epoch 132#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [131][100/100]\tTime 0.108 (0.121)\tData 0.076 (0.077)\tLoss 1.1064 (0.9616)\ttop1 70.000 (73.512)\n",
      "Test: [0/100]\tTime 0.405 (0.405)\tLoss 1.2162 (1.2162)\ttop1 72.000 (72.000)\n",
      " * top1 72.710\n",
      "#################epoch 133#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [132][100/100]\tTime 0.124 (0.115)\tData 0.087 (0.070)\tLoss 0.8778 (0.9600)\ttop1 74.800 (73.474)\n",
      "Test: [0/100]\tTime 0.407 (0.407)\tLoss 1.2240 (1.2240)\ttop1 73.000 (73.000)\n",
      " * top1 72.800\n",
      "#################epoch 134#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [133][100/100]\tTime 0.105 (0.116)\tData 0.074 (0.067)\tLoss 1.0312 (0.9543)\ttop1 73.000 (73.456)\n",
      "Test: [0/100]\tTime 0.387 (0.387)\tLoss 1.2178 (1.2178)\ttop1 73.000 (73.000)\n",
      " * top1 72.790\n",
      "#################epoch 135#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [134][100/100]\tTime 0.109 (0.121)\tData 0.073 (0.073)\tLoss 1.0312 (0.9578)\ttop1 72.800 (73.358)\n",
      "Test: [0/100]\tTime 0.386 (0.386)\tLoss 1.2224 (1.2224)\ttop1 72.000 (72.000)\n",
      " * top1 72.530\n",
      "#################epoch 136#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [135][100/100]\tTime 0.101 (0.118)\tData 0.069 (0.074)\tLoss 0.8944 (0.9400)\ttop1 76.800 (73.934)\n",
      "Test: [0/100]\tTime 0.393 (0.393)\tLoss 1.2074 (1.2074)\ttop1 71.000 (71.000)\n",
      " * top1 72.690\n",
      "#################epoch 137#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [136][100/100]\tTime 0.102 (0.120)\tData 0.068 (0.073)\tLoss 0.8874 (0.9512)\ttop1 74.400 (73.644)\n",
      "Test: [0/100]\tTime 0.363 (0.363)\tLoss 1.2055 (1.2055)\ttop1 72.000 (72.000)\n",
      " * top1 72.540\n",
      "#################epoch 138#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [137][100/100]\tTime 0.102 (0.119)\tData 0.068 (0.072)\tLoss 0.9076 (0.9520)\ttop1 74.400 (73.570)\n",
      "Test: [0/100]\tTime 0.371 (0.371)\tLoss 1.2084 (1.2084)\ttop1 71.000 (71.000)\n",
      " * top1 72.740\n",
      "#################epoch 139#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [138][100/100]\tTime 0.106 (0.117)\tData 0.073 (0.068)\tLoss 0.9932 (0.9550)\ttop1 72.600 (73.364)\n",
      "Test: [0/100]\tTime 0.377 (0.377)\tLoss 1.2109 (1.2109)\ttop1 71.000 (71.000)\n",
      " * top1 72.730\n",
      "#################epoch 140#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [139][100/100]\tTime 0.102 (0.117)\tData 0.070 (0.073)\tLoss 0.8633 (0.9430)\ttop1 74.600 (73.830)\n",
      "Test: [0/100]\tTime 0.376 (0.376)\tLoss 1.2135 (1.2135)\ttop1 71.000 (71.000)\n",
      " * top1 72.800\n",
      "#################epoch 141#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [140][100/100]\tTime 0.099 (0.120)\tData 0.067 (0.075)\tLoss 0.9567 (0.9521)\ttop1 71.600 (73.718)\n",
      "Test: [0/100]\tTime 0.380 (0.380)\tLoss 1.2073 (1.2073)\ttop1 71.000 (71.000)\n",
      " * top1 72.740\n",
      "#################epoch 142#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [141][100/100]\tTime 0.111 (0.122)\tData 0.075 (0.073)\tLoss 0.9866 (0.9495)\ttop1 74.400 (73.622)\n",
      "Test: [0/100]\tTime 0.367 (0.367)\tLoss 1.1935 (1.1935)\ttop1 72.000 (72.000)\n",
      " * top1 72.800\n",
      "#################epoch 143#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [142][100/100]\tTime 0.103 (0.117)\tData 0.071 (0.074)\tLoss 0.9617 (0.9365)\ttop1 72.200 (73.674)\n",
      "Test: [0/100]\tTime 0.379 (0.379)\tLoss 1.1978 (1.1978)\ttop1 72.000 (72.000)\n",
      " * top1 72.690\n",
      "#################epoch 144#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [143][100/100]\tTime 0.108 (0.116)\tData 0.073 (0.071)\tLoss 0.9597 (0.9518)\ttop1 75.600 (73.564)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.1995 (1.1995)\ttop1 72.000 (72.000)\n",
      " * top1 72.700\n",
      "#################epoch 145#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [144][100/100]\tTime 0.108 (0.115)\tData 0.072 (0.067)\tLoss 1.0597 (0.9491)\ttop1 70.200 (73.420)\n",
      "Test: [0/100]\tTime 0.371 (0.371)\tLoss 1.2038 (1.2038)\ttop1 71.000 (71.000)\n",
      " * top1 72.610\n",
      "#################epoch 146#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [145][100/100]\tTime 0.108 (0.117)\tData 0.071 (0.072)\tLoss 0.9690 (0.9506)\ttop1 72.600 (73.568)\n",
      "Test: [0/100]\tTime 0.380 (0.380)\tLoss 1.2086 (1.2086)\ttop1 72.000 (72.000)\n",
      " * top1 72.790\n",
      "#################epoch 147#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [146][100/100]\tTime 0.101 (0.120)\tData 0.069 (0.074)\tLoss 1.0410 (0.9418)\ttop1 70.400 (73.678)\n",
      "Test: [0/100]\tTime 0.378 (0.378)\tLoss 1.2123 (1.2123)\ttop1 71.000 (71.000)\n",
      " * top1 72.740\n",
      "#################epoch 148#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [147][100/100]\tTime 0.100 (0.115)\tData 0.070 (0.068)\tLoss 0.9739 (0.9492)\ttop1 72.000 (73.404)\n",
      "Test: [0/100]\tTime 0.399 (0.399)\tLoss 1.2122 (1.2122)\ttop1 71.000 (71.000)\n",
      " * top1 72.700\n",
      "#################epoch 149#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [148][100/100]\tTime 0.109 (0.121)\tData 0.073 (0.074)\tLoss 0.8669 (0.9435)\ttop1 74.200 (73.772)\n",
      "Test: [0/100]\tTime 0.383 (0.383)\tLoss 1.1930 (1.1930)\ttop1 73.000 (73.000)\n",
      " * top1 72.870\n",
      "#################epoch 150#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [149][100/100]\tTime 0.104 (0.116)\tData 0.071 (0.072)\tLoss 0.9213 (0.9431)\ttop1 75.600 (73.854)\n",
      "Test: [0/100]\tTime 0.384 (0.384)\tLoss 1.2066 (1.2066)\ttop1 71.000 (71.000)\n",
      " * top1 72.650\n",
      "#################epoch 151#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [150][100/100]\tTime 0.104 (0.122)\tData 0.070 (0.075)\tLoss 0.9818 (0.9395)\ttop1 72.000 (73.870)\n",
      "Test: [0/100]\tTime 0.404 (0.404)\tLoss 1.1966 (1.1966)\ttop1 73.000 (73.000)\n",
      " * top1 72.550\n",
      "#################epoch 152#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [151][100/100]\tTime 0.106 (0.120)\tData 0.071 (0.071)\tLoss 0.9165 (0.9545)\ttop1 74.200 (73.396)\n",
      "Test: [0/100]\tTime 0.379 (0.379)\tLoss 1.2001 (1.2001)\ttop1 72.000 (72.000)\n",
      " * top1 72.660\n",
      "#################epoch 153#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [152][100/100]\tTime 0.108 (0.116)\tData 0.073 (0.070)\tLoss 1.0028 (0.9445)\ttop1 70.600 (73.642)\n",
      "Test: [0/100]\tTime 0.398 (0.398)\tLoss 1.2109 (1.2109)\ttop1 72.000 (72.000)\n",
      " * top1 72.710\n",
      "#################epoch 154#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [153][100/100]\tTime 0.106 (0.118)\tData 0.070 (0.068)\tLoss 0.9557 (0.9379)\ttop1 72.000 (73.828)\n",
      "Test: [0/100]\tTime 0.374 (0.374)\tLoss 1.2110 (1.2110)\ttop1 72.000 (72.000)\n",
      " * top1 72.720\n",
      "#################epoch 155#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [154][100/100]\tTime 0.107 (0.121)\tData 0.072 (0.074)\tLoss 0.9028 (0.9426)\ttop1 75.000 (73.648)\n",
      "Test: [0/100]\tTime 0.369 (0.369)\tLoss 1.2135 (1.2135)\ttop1 71.000 (71.000)\n",
      " * top1 72.730\n",
      "#################epoch 156#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [155][100/100]\tTime 0.106 (0.116)\tData 0.071 (0.067)\tLoss 0.8737 (0.9467)\ttop1 73.600 (73.582)\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 1.2158 (1.2158)\ttop1 71.000 (71.000)\n",
      " * top1 72.880\n",
      "#################epoch 157#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [156][100/100]\tTime 0.107 (0.115)\tData 0.072 (0.071)\tLoss 1.0198 (0.9412)\ttop1 74.200 (73.772)\n",
      "Test: [0/100]\tTime 0.359 (0.359)\tLoss 1.2142 (1.2142)\ttop1 71.000 (71.000)\n",
      " * top1 72.680\n",
      "#################epoch 158#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [157][100/100]\tTime 0.105 (0.114)\tData 0.072 (0.070)\tLoss 0.9954 (0.9474)\ttop1 72.000 (73.580)\n",
      "Test: [0/100]\tTime 0.435 (0.435)\tLoss 1.2046 (1.2046)\ttop1 71.000 (71.000)\n",
      " * top1 72.710\n",
      "#################epoch 159#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [158][100/100]\tTime 0.104 (0.119)\tData 0.071 (0.069)\tLoss 0.9707 (0.9418)\ttop1 73.000 (73.752)\n",
      "Test: [0/100]\tTime 0.374 (0.374)\tLoss 1.2025 (1.2025)\ttop1 72.000 (72.000)\n",
      " * top1 72.800\n",
      "#################epoch 160#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [159][100/100]\tTime 0.100 (0.121)\tData 0.067 (0.074)\tLoss 0.8261 (0.9484)\ttop1 76.000 (73.652)\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 1.2002 (1.2002)\ttop1 72.000 (72.000)\n",
      " * top1 72.780\n",
      "#################epoch 161#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [160][100/100]\tTime 0.105 (0.121)\tData 0.070 (0.075)\tLoss 0.9925 (0.9462)\ttop1 74.800 (73.898)\n",
      "Test: [0/100]\tTime 0.378 (0.378)\tLoss 1.2178 (1.2178)\ttop1 72.000 (72.000)\n",
      " * top1 72.860\n",
      "#################epoch 162#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [161][100/100]\tTime 0.102 (0.115)\tData 0.068 (0.066)\tLoss 0.9456 (0.9444)\ttop1 73.600 (73.704)\n",
      "Test: [0/100]\tTime 0.368 (0.368)\tLoss 1.1935 (1.1935)\ttop1 73.000 (73.000)\n",
      " * top1 72.680\n",
      "#################epoch 163#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [162][100/100]\tTime 0.099 (0.115)\tData 0.066 (0.067)\tLoss 0.8359 (0.9371)\ttop1 79.800 (73.930)\n",
      "Test: [0/100]\tTime 0.356 (0.356)\tLoss 1.2029 (1.2029)\ttop1 72.000 (72.000)\n",
      " * top1 72.680\n",
      "#################epoch 164#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [163][100/100]\tTime 0.105 (0.116)\tData 0.070 (0.070)\tLoss 0.9367 (0.9487)\ttop1 73.800 (73.810)\n",
      "Test: [0/100]\tTime 0.354 (0.354)\tLoss 1.2085 (1.2085)\ttop1 71.000 (71.000)\n",
      " * top1 72.690\n",
      "#################epoch 165#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [164][100/100]\tTime 0.100 (0.115)\tData 0.065 (0.068)\tLoss 0.9291 (0.9431)\ttop1 74.000 (73.612)\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 1.2059 (1.2059)\ttop1 71.000 (71.000)\n",
      " * top1 72.610\n",
      "#################epoch 166#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [165][100/100]\tTime 0.097 (0.114)\tData 0.065 (0.075)\tLoss 0.9295 (0.9427)\ttop1 74.200 (73.938)\n",
      "Test: [0/100]\tTime 0.372 (0.372)\tLoss 1.2002 (1.2002)\ttop1 71.000 (71.000)\n",
      " * top1 72.770\n",
      "#################epoch 167#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [166][100/100]\tTime 0.101 (0.115)\tData 0.065 (0.070)\tLoss 0.9117 (0.9356)\ttop1 75.800 (73.832)\n",
      "Test: [0/100]\tTime 0.347 (0.347)\tLoss 1.1850 (1.1850)\ttop1 71.000 (71.000)\n",
      " * top1 72.810\n",
      "#################epoch 168#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [167][100/100]\tTime 0.103 (0.115)\tData 0.068 (0.070)\tLoss 0.9404 (0.9356)\ttop1 74.200 (73.924)\n",
      "Test: [0/100]\tTime 0.364 (0.364)\tLoss 1.2035 (1.2035)\ttop1 72.000 (72.000)\n",
      " * top1 72.900\n",
      "#################epoch 169#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [168][100/100]\tTime 0.098 (0.120)\tData 0.063 (0.073)\tLoss 0.9503 (0.9407)\ttop1 74.000 (73.816)\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 1.2020 (1.2020)\ttop1 71.000 (71.000)\n",
      " * top1 72.780\n",
      "#################epoch 170#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [169][100/100]\tTime 0.098 (0.115)\tData 0.065 (0.069)\tLoss 1.0380 (0.9452)\ttop1 71.200 (73.614)\n",
      "Test: [0/100]\tTime 0.361 (0.361)\tLoss 1.2096 (1.2096)\ttop1 71.000 (71.000)\n",
      " * top1 72.810\n",
      "#################epoch 171#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [170][100/100]\tTime 0.099 (0.115)\tData 0.065 (0.070)\tLoss 1.0255 (0.9434)\ttop1 72.000 (73.906)\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 1.2127 (1.2127)\ttop1 71.000 (71.000)\n",
      " * top1 72.940\n",
      "#################epoch 172#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [171][100/100]\tTime 0.099 (0.116)\tData 0.066 (0.067)\tLoss 0.9655 (0.9415)\ttop1 72.000 (73.780)\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 1.2079 (1.2079)\ttop1 71.000 (71.000)\n",
      " * top1 72.820\n",
      "#################epoch 173#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [172][100/100]\tTime 0.095 (0.114)\tData 0.063 (0.070)\tLoss 0.9650 (0.9466)\ttop1 75.000 (73.958)\n",
      "Test: [0/100]\tTime 0.355 (0.355)\tLoss 1.2185 (1.2185)\ttop1 71.000 (71.000)\n",
      " * top1 72.740\n",
      "#################epoch 174#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [173][100/100]\tTime 0.095 (0.122)\tData 0.064 (0.076)\tLoss 0.8767 (0.9464)\ttop1 75.000 (73.718)\n",
      "Test: [0/100]\tTime 0.357 (0.357)\tLoss 1.1989 (1.1989)\ttop1 72.000 (72.000)\n",
      " * top1 72.910\n",
      "#################epoch 175#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [174][100/100]\tTime 0.100 (0.117)\tData 0.067 (0.071)\tLoss 0.8815 (0.9393)\ttop1 74.600 (73.818)\n",
      "Test: [0/100]\tTime 0.357 (0.357)\tLoss 1.2015 (1.2015)\ttop1 72.000 (72.000)\n",
      " * top1 72.880\n",
      "#################epoch 176#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [175][100/100]\tTime 0.100 (0.120)\tData 0.067 (0.072)\tLoss 0.8634 (0.9431)\ttop1 74.200 (73.824)\n",
      "Test: [0/100]\tTime 0.391 (0.391)\tLoss 1.2148 (1.2148)\ttop1 71.000 (71.000)\n",
      " * top1 72.950\n",
      "#################epoch 177#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [176][100/100]\tTime 0.107 (0.122)\tData 0.074 (0.075)\tLoss 0.8703 (0.9376)\ttop1 74.200 (73.664)\n",
      "Test: [0/100]\tTime 0.359 (0.359)\tLoss 1.2098 (1.2098)\ttop1 73.000 (73.000)\n",
      " * top1 72.920\n",
      "#################epoch 178#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [177][100/100]\tTime 0.093 (0.115)\tData 0.063 (0.067)\tLoss 0.9580 (0.9402)\ttop1 74.000 (73.860)\n",
      "Test: [0/100]\tTime 0.355 (0.355)\tLoss 1.2023 (1.2023)\ttop1 72.000 (72.000)\n",
      " * top1 72.940\n",
      "#################epoch 179#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [178][100/100]\tTime 0.105 (0.120)\tData 0.070 (0.076)\tLoss 0.9303 (0.9328)\ttop1 73.200 (73.946)\n",
      "Test: [0/100]\tTime 0.342 (0.342)\tLoss 1.2003 (1.2003)\ttop1 72.000 (72.000)\n",
      " * top1 72.710\n",
      "#################epoch 180#################\n",
      "lr=0.001000 \n",
      "\n",
      "Epoch: [179][100/100]\tTime 0.102 (0.115)\tData 0.067 (0.070)\tLoss 0.9018 (0.9339)\ttop1 74.400 (73.668)\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 1.2025 (1.2025)\ttop1 72.000 (72.000)\n",
      " * top1 72.830\n",
      "#################epoch 181#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [180][100/100]\tTime 0.099 (0.115)\tData 0.066 (0.070)\tLoss 0.9482 (0.9255)\ttop1 75.600 (74.258)\n",
      "Test: [0/100]\tTime 0.372 (0.372)\tLoss 1.1988 (1.1988)\ttop1 72.000 (72.000)\n",
      " * top1 72.830\n",
      "#################epoch 182#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [181][100/100]\tTime 0.106 (0.117)\tData 0.066 (0.067)\tLoss 0.9701 (0.9354)\ttop1 74.000 (74.110)\n",
      "Test: [0/100]\tTime 0.351 (0.351)\tLoss 1.2042 (1.2042)\ttop1 72.000 (72.000)\n",
      " * top1 72.780\n",
      "#################epoch 183#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [182][100/100]\tTime 0.104 (0.123)\tData 0.068 (0.074)\tLoss 0.9983 (0.9433)\ttop1 72.400 (73.934)\n",
      "Test: [0/100]\tTime 0.348 (0.348)\tLoss 1.2035 (1.2035)\ttop1 72.000 (72.000)\n",
      " * top1 72.760\n",
      "#################epoch 184#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [183][100/100]\tTime 0.097 (0.115)\tData 0.065 (0.066)\tLoss 1.0084 (0.9302)\ttop1 72.200 (74.046)\n",
      "Test: [0/100]\tTime 0.350 (0.350)\tLoss 1.2039 (1.2039)\ttop1 72.000 (72.000)\n",
      " * top1 72.810\n",
      "#################epoch 185#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [184][100/100]\tTime 0.102 (0.122)\tData 0.067 (0.077)\tLoss 0.9739 (0.9437)\ttop1 71.800 (73.580)\n",
      "Test: [0/100]\tTime 0.337 (0.337)\tLoss 1.2082 (1.2082)\ttop1 72.000 (72.000)\n",
      " * top1 72.810\n",
      "#################epoch 186#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [185][100/100]\tTime 0.093 (0.116)\tData 0.062 (0.067)\tLoss 0.9589 (0.9226)\ttop1 73.000 (74.514)\n",
      "Test: [0/100]\tTime 0.383 (0.383)\tLoss 1.2098 (1.2098)\ttop1 72.000 (72.000)\n",
      " * top1 72.850\n",
      "#################epoch 187#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [186][100/100]\tTime 0.105 (0.123)\tData 0.068 (0.073)\tLoss 0.9256 (0.9315)\ttop1 74.200 (74.152)\n",
      "Test: [0/100]\tTime 0.379 (0.379)\tLoss 1.2022 (1.2022)\ttop1 72.000 (72.000)\n",
      " * top1 72.800\n",
      "#################epoch 188#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [187][100/100]\tTime 0.104 (0.117)\tData 0.069 (0.068)\tLoss 0.9256 (0.9380)\ttop1 74.800 (73.684)\n",
      "Test: [0/100]\tTime 0.360 (0.360)\tLoss 1.2037 (1.2037)\ttop1 72.000 (72.000)\n",
      " * top1 72.840\n",
      "#################epoch 189#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [188][100/100]\tTime 0.100 (0.115)\tData 0.066 (0.069)\tLoss 0.9417 (0.9433)\ttop1 73.000 (73.926)\n",
      "Test: [0/100]\tTime 0.345 (0.345)\tLoss 1.2107 (1.2107)\ttop1 72.000 (72.000)\n",
      " * top1 72.850\n",
      "#################epoch 190#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [189][100/100]\tTime 0.102 (0.115)\tData 0.068 (0.071)\tLoss 0.9252 (0.9328)\ttop1 73.800 (74.182)\n",
      "Test: [0/100]\tTime 0.343 (0.343)\tLoss 1.2128 (1.2128)\ttop1 72.000 (72.000)\n",
      " * top1 72.810\n",
      "#################epoch 191#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [190][100/100]\tTime 0.094 (0.115)\tData 0.061 (0.065)\tLoss 0.9488 (0.9441)\ttop1 73.600 (73.954)\n",
      "Test: [0/100]\tTime 0.353 (0.353)\tLoss 1.2073 (1.2073)\ttop1 73.000 (73.000)\n",
      " * top1 72.840\n",
      "#################epoch 192#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [191][100/100]\tTime 0.098 (0.116)\tData 0.066 (0.067)\tLoss 0.8125 (0.9257)\ttop1 78.200 (74.462)\n",
      "Test: [0/100]\tTime 0.347 (0.347)\tLoss 1.2006 (1.2006)\ttop1 72.000 (72.000)\n",
      " * top1 72.830\n",
      "#################epoch 193#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [192][100/100]\tTime 0.098 (0.114)\tData 0.065 (0.069)\tLoss 0.9669 (0.9465)\ttop1 72.400 (73.596)\n",
      "Test: [0/100]\tTime 0.355 (0.355)\tLoss 1.2025 (1.2025)\ttop1 72.000 (72.000)\n",
      " * top1 72.840\n",
      "#################epoch 194#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [193][100/100]\tTime 0.097 (0.118)\tData 0.068 (0.073)\tLoss 0.8813 (0.9366)\ttop1 76.600 (73.976)\n",
      "Test: [0/100]\tTime 0.357 (0.357)\tLoss 1.2110 (1.2110)\ttop1 71.000 (71.000)\n",
      " * top1 72.700\n",
      "#################epoch 195#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [194][100/100]\tTime 0.094 (0.115)\tData 0.062 (0.069)\tLoss 0.8453 (0.9305)\ttop1 78.000 (73.868)\n",
      "Test: [0/100]\tTime 0.356 (0.356)\tLoss 1.2039 (1.2039)\ttop1 72.000 (72.000)\n",
      " * top1 72.750\n",
      "#################epoch 196#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [195][100/100]\tTime 0.101 (0.118)\tData 0.069 (0.075)\tLoss 0.9678 (0.9380)\ttop1 74.400 (74.044)\n",
      "Test: [0/100]\tTime 0.366 (0.366)\tLoss 1.2026 (1.2026)\ttop1 72.000 (72.000)\n",
      " * top1 72.930\n",
      "#################epoch 197#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [196][100/100]\tTime 0.096 (0.116)\tData 0.062 (0.068)\tLoss 0.8682 (0.9342)\ttop1 77.800 (73.796)\n",
      "Test: [0/100]\tTime 0.340 (0.340)\tLoss 1.1999 (1.1999)\ttop1 72.000 (72.000)\n",
      " * top1 72.790\n",
      "#################epoch 198#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [197][100/100]\tTime 0.102 (0.114)\tData 0.067 (0.065)\tLoss 0.9217 (0.9367)\ttop1 73.600 (74.054)\n",
      "Test: [0/100]\tTime 0.347 (0.347)\tLoss 1.2108 (1.2108)\ttop1 72.000 (72.000)\n",
      " * top1 72.810\n",
      "#################epoch 199#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [198][100/100]\tTime 0.097 (0.119)\tData 0.062 (0.070)\tLoss 1.0118 (0.9420)\ttop1 69.600 (73.854)\n",
      "Test: [0/100]\tTime 0.338 (0.338)\tLoss 1.2025 (1.2025)\ttop1 72.000 (72.000)\n",
      " * top1 72.920\n",
      "#################epoch 200#################\n",
      "lr=0.000100 \n",
      "\n",
      "Epoch: [199][100/100]\tTime 0.097 (0.123)\tData 0.064 (0.075)\tLoss 0.8870 (0.9325)\ttop1 76.000 (74.044)\n",
      "Test: [0/100]\tTime 0.355 (0.355)\tLoss 1.2120 (1.2120)\ttop1 72.000 (72.000)\n",
      " * top1 72.850\n",
      "save model weight to ./models/stl_vgg64_7_plane-cifar100-loss-1.0796589195728301-top1-72.94999694824219.pth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(f\"{os.environ['HOME']}/packages/autoqnn\")\n",
    "import autoqnn\n",
    "# 将程序的输出重定向到文件  \n",
    "# file_path = f'./training_logs/{model_name}-on-cifar100-lr-{lr}-wd-{weight_decay}-epoch-{epochs}.txt'  \n",
    "# sys.stdout = open(file_path, 'w')\n",
    "\n",
    "# 要输出的内容  \n",
    "for epoch in range(epochs):\n",
    "    # adjust_learning_rate(optim,epoch,steps=[30,60])\n",
    "    print(f\"#################epoch {epoch+1}#################\")\n",
    "    print(\"lr=%.6f \\n\"%(optim.param_groups[0]['lr']))\n",
    "    batch_time[epoch],data_time[epoch],loss[epoch],metric=autoqnn.core.train(trainloader,model,criterion,optim,\n",
    "                       metric_meds=[autoqnn.core.top1],epoch=epoch)\n",
    "    metrics[epoch]=metric[0].item()\n",
    "    \n",
    "    print()\n",
    "    val_loss[epoch],metric = autoqnn.core.validate(testloader,model,criterion,\n",
    "                      metric_meds=[autoqnn.core.top1],\n",
    "                      print_freq=100)\n",
    "    val_metrics[epoch] = metric[0].item()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Save the trained parameters to disk\n",
    "best_loss = np.min(val_loss)\n",
    "best_metric = np.max(val_metrics)\n",
    "save_file_name = f\"./models/{model_name}-cifar100-loss-{best_loss}-top1-{best_metric}.pth\"\n",
    "torch.save(model.state_dict(),save_file_name)\n",
    "print(f\"save model weight to {save_file_name}\")\n",
    "# 关闭文件  \n",
    "# sys.stdout.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b4c1f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the training curve\n",
    "from train_utils import display_loss_plot\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(10,5))\n",
    "# Plot training loss over epochs\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "plt.subplot(1,2,1)\n",
    "display_loss_plot(loss_per_epoch)\n",
    "# Plot test accuracy over epochs\n",
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "plt.subplot(1,2,2)\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00e6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained parameters to disk\n",
    "torch.save(model.state_dict(), \n",
    "    \"/workspace/notebooks/FSS/models/resnet18-cifar100-SDN-66.14-71.95-76.54-76.82.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
