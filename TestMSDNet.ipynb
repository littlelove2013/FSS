{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb24ef73-b23b-4f65-9498-2154a1b90964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11011MB, multi_processor_count=68)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743031d-3749-463c-b98b-549967a4bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../packages/autoqnn/\")\n",
    "import autoqnn\n",
    "from dns_msdnet import msdnet_cifar100, msdnet_imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163249-ee08-4701-8bc6-ba8d48adb76e",
   "metadata": {},
   "source": [
    "# DNS MSDNet \n",
    "测试DNS在MSDNet上的效果如何，首先是在Cifar100数据集上的效果，先测试一下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d6e195-fb46-47af-82c1-3139f2ae75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building network of steps: \n",
      "[4, 2, 2, 2, 2, 2, 2] 16\n",
      " ********************** Block 1  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 16 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 32 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 48 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 2  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 3  **********************\n",
      "|\t\tinScales 3 outScales 2 inChannels 112 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 128, outChannels 64\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 4  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 5  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 112 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 128 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 6  **********************\n",
      "|\t\tinScales 2 outScales 1 inChannels 144 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 160, outChannels 80\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 7  **********************\n",
      "|\t\tinScales 1 outScales 1 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 112 outChannels 16\t\t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = msdnet_cifar100().cuda()\n",
    "model_name = \"msdnet_cifar100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b022059f-136a-4ce6-b783-62f47acf1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b230e18-0505-4376-b326-bd28223a4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "from autoqnn import datasets\n",
    "torch_weights_path=f\"{os.environ['HOME']}/models/torch/weights\"\n",
    "data_root=\"~/datasets/cifar100\"\n",
    "dataset_name = 'cifar100'\n",
    "trainloader,testloader,classes = datasets.cifar.get_cifar_dataloader(\n",
    "    root=data_root,dataset=dataset_name,autoaugment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c45022ef-1dc0-4e75-96e0-da374489af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, temperature, gamma, nBlocks):\n",
    "        super(KDLoss, self).__init__()\n",
    "        \n",
    "        self.kld_loss = nn.KLDivLoss().cuda()\n",
    "        self.ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "        self.softmax = nn.Softmax(dim=1).cuda()\n",
    "\n",
    "        self.T = temperature\n",
    "        self.gamma = gamma\n",
    "        self.nBlocks = nBlocks\n",
    "\n",
    "    def loss_fn_kd(self, outputs, targets, soft_targets):\n",
    "        loss = self.ce_loss(outputs[-1], targets)\n",
    "        T = self.T\n",
    "        for i in range(self.nBlocks - 1):\n",
    "            _ce = (1. - self.gamma) * self.ce_loss(outputs[i], targets)\n",
    "            _kld = self.kld_loss(self.log_softmax(outputs[i] / T), self.softmax(soft_targets.detach() / T)) * self.gamma * T * T\n",
    "            loss = loss + _ce + _kld\n",
    "        return loss\n",
    "\n",
    "def train(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq=10):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    running_lr = None\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=i,\n",
    "                                  nBatch=len(train_loader), method=\"multistep\")\n",
    "        # measure data loading time\n",
    "        if running_lr is None:\n",
    "            running_lr = lr\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output, middle_feas = model(input_var)\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "\n",
    "        loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        for j in range(len(output)):\n",
    "            acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "            top1[j].update(acc1.item(), input.size(0))\n",
    "            top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.avg:.3f}\\t'\n",
    "                  'Data {data_time.avg:.3f}\\t'\n",
    "                  'Loss {loss.val:.4f}\\t'\n",
    "                  'Acc@1 {top1.val:.4f}\\t'\n",
    "                  'Acc@5 {top5.val:.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader),\n",
    "                    batch_time=batch_time, data_time=data_time,\n",
    "                    loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg, running_lr\n",
    "\n",
    "def validate(val_loader, model, kd_loss,print_freq=10):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output,_ = model(input_var)\n",
    "            if not isinstance(output, list):\n",
    "                output = [output]\n",
    "\n",
    "            loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "\n",
    "            # measure error and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "\n",
    "            for j in range(len(output)):\n",
    "                acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "                top1[j].update(acc1.item(), input.size(0))\n",
    "                top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.avg:.3f}\\t'\n",
    "                      'Data {data_time.avg:.3f}\\t'\n",
    "                      'Loss {loss.val:.4f}\\t'\n",
    "                      'Acc@1 {top1.val:.4f}\\t'\n",
    "                      'Acc@5 {top5.val:.4f}'.format(\n",
    "                        i + 1, len(val_loader),\n",
    "                        batch_time=batch_time, data_time=data_time,\n",
    "                        loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "                # break\n",
    "    for j in range(model.nBlocks):\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1[j], top5=top5[j]))\n",
    "        \"\"\"\n",
    "        print('Exit {}\\t'\n",
    "              'Err@1 {:.4f}\\t'\n",
    "              'Err@5 {:.4f}'.format(\n",
    "              j, top1[j].avg, top5[j].avg))\n",
    "        \"\"\"\n",
    "    # print(' * Err@1 {top1.avg:.3f} Err@5 {top5.avg:.3f}'.format(top1=top1[-1], top5=top5[-1]))\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best, filename, result):\n",
    "    print(save_path)\n",
    "    result_filename = os.path.join(save_path, 'scores.tsv')\n",
    "    model_dir = os.path.join(save_path, 'save_models')\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    model_filename = os.path.join(model_dir, filename)\n",
    "    best_filename = os.path.join(model_dir, 'model_best.pth.tar')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"=> saving checkpoint '{}'\".format(model_filename))\n",
    "\n",
    "    torch.save(state, model_filename)\n",
    "\n",
    "    with open(result_filename, 'w') as f:\n",
    "        print('\\n'.join(result), file=f)\n",
    "\n",
    "    with open(latest_filename, 'w') as fout:\n",
    "        fout.write(model_filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_filename, best_filename)\n",
    "\n",
    "    print(\"=> saved checkpoint '{}'\".format(model_filename))\n",
    "    return\n",
    "\n",
    "def load_checkpoint(save_path):\n",
    "    model_dir = save_path\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    if os.path.exists(latest_filename):\n",
    "        with open(latest_filename, 'r') as fin:\n",
    "            model_filename = fin.readlines()[0]\n",
    "    else:\n",
    "        return None\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_filename))\n",
    "    state = torch.load(model_filename)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(model_filename))\n",
    "    return state\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the error@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        # res.append(100.0 - correct_k.mul_(100.0 / batch_size))\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "import math\n",
    "def adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=None,\n",
    "                         nBatch=None, method='multistep'):\n",
    "    if method == 'cosine':\n",
    "        T_total = epochs * nBatch\n",
    "        T_cur = (epoch % epochs) * nBatch + batch\n",
    "        lr = 0.5 * init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n",
    "    elif method == 'multistep':\n",
    "        data = \"cifar100\"\n",
    "        if data.startswith('cifar'):\n",
    "            lr, decay_rate = init_lr, 0.1\n",
    "            if epoch >= epochs * 0.75:\n",
    "                lr *= decay_rate ** 2\n",
    "            elif epoch >= epochs * 0.5:\n",
    "                lr *= decay_rate\n",
    "        else:\n",
    "            lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557cb254-dd16-4641-9fa5-c8f292286396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code\n",
    "init_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "epochs = 200\n",
    "temperature = 3.0\n",
    "gamma = 0.9\n",
    "nBlocks = 7\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "kd_loss = KDLoss(temperature, gamma, nBlocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f8e69-1a3b-42da-bc29-2b5df2aa34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['epoch\\tlr\\ttrain_loss\\tval_loss\\ttrain_acc1'\n",
    "              '\\tval_acc1\\ttrain_acc5\\tval_acc5']\n",
    "\n",
    "import sys\n",
    "import time\n",
    "# 将程序的输出重定向到文件  \n",
    "file_path = f'./training_logs/{model_name}-on-cifar100-lr-{init_lr}-wd-{weight_decay}-epoch-{epochs}.txt'  \n",
    "sys.stdout = open(file_path, 'w')\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    train_loss, train_acc1, train_acc5, lr = train(trainloader, model, kd_loss, optimizer, epoch,epochs,init_lr)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_acc1, val_acc5 = validate(testloader, model, kd_loss)\n",
    "\n",
    "    # save scores to a tsv file, rewrite the whole file to prevent\n",
    "    # accidental deletion\n",
    "    scores.append(('{}\\t{:.3f}' + '\\t{:.4f}' * 6)\n",
    "                  .format(epoch, lr, train_loss, val_loss,\n",
    "                          train_acc1, val_acc1, train_acc5, val_acc5))\n",
    "\n",
    "    is_best = val_acc1 > best_acc1\n",
    "    if is_best:\n",
    "        best_acc1 = val_acc1\n",
    "        best_epoch = epoch\n",
    "        print('Best var_acc1 {}'.format(best_acc1))\n",
    "\n",
    "    save_path = f\"./models/{model_name}-cifar100/\"\n",
    "    model_filename = 'checkpoint_%03d.pth.tar' % epoch\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'arch': model_name,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, save_path, is_best, model_filename, scores)\n",
    "\n",
    "print('Best val_acc1: {:.4f} at epoch {}'.format(best_acc1, best_epoch))\n",
    "\n",
    "# Save the trained parameters to disk\n",
    "# best_loss = np.min(val_loss)\n",
    "# best_metric = np.max(val_metrics）\n",
    "save_file_name = f\"./models/{model_name}-cifar100-top1-{best_acc1}.pth\"\n",
    "torch.save(model.state_dict(),save_file_name)\n",
    "print(f\"save model weight to {save_file_name}\")\n",
    "# 关闭文件  \n",
    "sys.stdout.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a8fe97-e117-4d95-9398-f8b1b374d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/100]\tTime 1.063\tData 0.907\tLoss 6.1577\tAcc@1 13.2000\tAcc@5 36.6000\n",
      "Epoch: [1][11/100]\tTime 0.391\tData 0.086\tLoss 6.1149\tAcc@1 13.6000\tAcc@5 37.6000\n",
      "Epoch: [1][21/100]\tTime 0.364\tData 0.047\tLoss 6.1194\tAcc@1 13.4000\tAcc@5 40.4000\n",
      "Epoch: [1][31/100]\tTime 0.356\tData 0.033\tLoss 6.1191\tAcc@1 14.8000\tAcc@5 37.2000\n",
      "Epoch: [1][41/100]\tTime 0.353\tData 0.026\tLoss 6.0268\tAcc@1 16.6000\tAcc@5 41.8000\n",
      "Epoch: [1][51/100]\tTime 0.356\tData 0.022\tLoss 6.0687\tAcc@1 12.8000\tAcc@5 35.8000\n",
      "Epoch: [1][61/100]\tTime 0.376\tData 0.019\tLoss 5.9661\tAcc@1 16.8000\tAcc@5 42.0000\n",
      "Epoch: [1][71/100]\tTime 0.397\tData 0.017\tLoss 5.9588\tAcc@1 16.6000\tAcc@5 42.2000\n",
      "Epoch: [1][81/100]\tTime 0.413\tData 0.015\tLoss 5.9773\tAcc@1 15.0000\tAcc@5 37.8000\n",
      "Epoch: [1][91/100]\tTime 0.425\tData 0.014\tLoss 6.0855\tAcc@1 14.2000\tAcc@5 36.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.0535579013824465, 14.726000289916993, 38.98800092697144, 0.1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trainloader, model, kd_loss, optimizer, 1,epochs,init_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
