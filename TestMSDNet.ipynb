{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb24ef73-b23b-4f65-9498-2154a1b90964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 可用\n",
      "共有1个 GPU 设备\n",
      "设备0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3090', major=8, minor=6, total_memory=24259MB, multi_processor_count=82)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "# 检查 GPU 是否可用  \n",
    "import torch\n",
    "if torch.cuda.is_available():  \n",
    "   print(\"GPU 可用\")  \n",
    "   # 获取 GPU 设备数量  \n",
    "   num_gpus = torch.cuda.device_count()  \n",
    "   print(f\"共有{num_gpus}个 GPU 设备\")\n",
    "   # 获取 GPU 设备信息  \n",
    "   for i in range(num_gpus):  \n",
    "       print(f\"设备{i}: {torch.cuda.get_device_properties(i)}\")  \n",
    "else:  \n",
    "   print(\"GPU 不可用\")  \n",
    "import sys\n",
    "# sys.path.append(\"/workspace/notebooks/\")\n",
    "# sys.path.append(\"/workspace/notebooks/FSS/\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743031d-3749-463c-b98b-549967a4bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../packages/autoqnn/\")\n",
    "import autoqnn\n",
    "from dns_msdnet import msdnet_cifar100, msdnet_imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11163249-ee08-4701-8bc6-ba8d48adb76e",
   "metadata": {},
   "source": [
    "# DNS MSDNet \n",
    "测试DNS在MSDNet上的效果如何，首先是在Cifar100数据集上的效果，先测试一下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d6e195-fb46-47af-82c1-3139f2ae75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building network of steps: \n",
      "[4, 2, 2, 2, 2, 2, 2] 16\n",
      " ********************** Block 1  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 16 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 32 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 48 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 2  **********************\n",
      "|\t\tinScales 3 outScales 3 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 3 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 3  **********************\n",
      "|\t\tinScales 3 outScales 2 inChannels 112 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 128, outChannels 64\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 64 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 4  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 5  **********************\n",
      "|\t\tinScales 2 outScales 2 inChannels 112 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 2 inChannels 128 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 6  **********************\n",
      "|\t\tinScales 2 outScales 1 inChannels 144 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 160, outChannels 80\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 80 outChannels 16\t\t|\n",
      "\n",
      " ********************** Block 7  **********************\n",
      "|\t\tinScales 1 outScales 1 inChannels 96 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 1 outScales 1 inChannels 112 outChannels 16\t\t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = msdnet_cifar100().cuda()\n",
    "model_name = \"msdnet_cifar100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b022059f-136a-4ce6-b783-62f47acf1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b230e18-0505-4376-b326-bd28223a4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "from autoqnn import datasets\n",
    "torch_weights_path=f\"{os.environ['HOME']}/models/torch/weights\"\n",
    "data_root=\"~/datasets/cifar100\"\n",
    "dataset_name = 'cifar100'\n",
    "trainloader,testloader,classes = datasets.cifar.get_cifar_dataloader(\n",
    "    root=data_root,dataset=dataset_name,autoaugment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "557cb254-dd16-4641-9fa5-c8f292286396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code\n",
    "init_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "epochs = 200\n",
    "temperature = 3.0\n",
    "gamma = 0.9\n",
    "nBlocks = 7\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                init_lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "kd_loss = KDLoss(temperature, gamma, nBlocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c45022ef-1dc0-4e75-96e0-da374489af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, temperature, gamma, nBlocks):\n",
    "        super(KDLoss, self).__init__()\n",
    "        \n",
    "        self.kld_loss = nn.KLDivLoss().cuda()\n",
    "        self.ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "        self.softmax = nn.Softmax(dim=1).cuda()\n",
    "\n",
    "        self.T = temperature\n",
    "        self.gamma = gamma\n",
    "        self.nBlocks = nBlocks\n",
    "\n",
    "    def loss_fn_kd(self, outputs, targets, soft_targets):\n",
    "        loss = self.ce_loss(outputs[-1], targets)\n",
    "        T = self.T\n",
    "        for i in range(self.nBlocks - 1):\n",
    "            _ce = (1. - self.gamma) * self.ce_loss(outputs[i], targets)\n",
    "            _kld = self.kld_loss(self.log_softmax(outputs[i] / T), self.softmax(soft_targets.detach() / T)) * self.gamma * T * T\n",
    "            loss = loss + _ce + _kld\n",
    "        return loss\n",
    "\n",
    "def train(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq=100):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    running_lr = None\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        lr = adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=i,\n",
    "                                  nBatch=len(train_loader), method=\"multistep\")\n",
    "        # measure data loading time\n",
    "        if running_lr is None:\n",
    "            running_lr = lr\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output, soft_target = model(input_var)\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "\n",
    "        loss = kd_loss.loss_fn_kd(output, target_var, soft_target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        for j in range(len(output)):\n",
    "            acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "            top1[j].update(acc1.item(), input.size(0))\n",
    "            top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.avg:.3f}\\t'\n",
    "                  'Data {data_time.avg:.3f}\\t'\n",
    "                  'Loss {loss.val:.4f}\\t'\n",
    "                  'Acc@1 {top1.val:.4f}\\t'\n",
    "                  'Acc@5 {top5.val:.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader),\n",
    "                    batch_time=batch_time, data_time=data_time,\n",
    "                    loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg, running_lr\n",
    "\n",
    "def validate(val_loader, model, kd_loss,print_freq=100):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    top1, top5 = [], []\n",
    "    for i in range(model.nBlocks):\n",
    "        top1.append(AverageMeter())\n",
    "        top5.append(AverageMeter())\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            if not isinstance(output, list):\n",
    "                output = [output]\n",
    "\n",
    "            loss = kd_loss.loss_fn_kd(output, target_var, output[-1])\n",
    "\n",
    "            # measure error and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "\n",
    "            for j in range(len(output)):\n",
    "                acc1, acc5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "                top1[j].update(acc1.item(), input.size(0))\n",
    "                top5[j].update(acc5.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.avg:.3f}\\t'\n",
    "                      'Data {data_time.avg:.3f}\\t'\n",
    "                      'Loss {loss.val:.4f}\\t'\n",
    "                      'Acc@1 {top1.val:.4f}\\t'\n",
    "                      'Acc@5 {top5.val:.4f}'.format(\n",
    "                        i + 1, len(val_loader),\n",
    "                        batch_time=batch_time, data_time=data_time,\n",
    "                        loss=losses, top1=top1[-1], top5=top5[-1]))\n",
    "                # break\n",
    "    for j in range(model.nBlocks):\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1[j], top5=top5[j]))\n",
    "        \"\"\"\n",
    "        print('Exit {}\\t'\n",
    "              'Err@1 {:.4f}\\t'\n",
    "              'Err@5 {:.4f}'.format(\n",
    "              j, top1[j].avg, top5[j].avg))\n",
    "        \"\"\"\n",
    "    # print(' * Err@1 {top1.avg:.3f} Err@5 {top5.avg:.3f}'.format(top1=top1[-1], top5=top5[-1]))\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best, filename, result):\n",
    "    print(save_path)\n",
    "    result_filename = os.path.join(save_path, 'scores.tsv')\n",
    "    model_dir = os.path.join(save_path, 'save_models')\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    model_filename = os.path.join(model_dir, filename)\n",
    "    best_filename = os.path.join(model_dir, 'model_best.pth.tar')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"=> saving checkpoint '{}'\".format(model_filename))\n",
    "\n",
    "    torch.save(state, model_filename)\n",
    "\n",
    "    with open(result_filename, 'w') as f:\n",
    "        print('\\n'.join(result), file=f)\n",
    "\n",
    "    with open(latest_filename, 'w') as fout:\n",
    "        fout.write(model_filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(model_filename, best_filename)\n",
    "\n",
    "    print(\"=> saved checkpoint '{}'\".format(model_filename))\n",
    "    return\n",
    "\n",
    "def load_checkpoint(save_path):\n",
    "    model_dir = save_path\n",
    "    latest_filename = os.path.join(model_dir, 'latest.txt')\n",
    "    if os.path.exists(latest_filename):\n",
    "        with open(latest_filename, 'r') as fin:\n",
    "            model_filename = fin.readlines()[0]\n",
    "    else:\n",
    "        return None\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_filename))\n",
    "    state = torch.load(model_filename)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(model_filename))\n",
    "    return state\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the error@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        # res.append(100.0 - correct_k.mul_(100.0 / batch_size))\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "import math\n",
    "def adjust_learning_rate(optimizer, epoch, epochs, init_lr, batch=None,\n",
    "                         nBatch=None, method='multistep'):\n",
    "    if method == 'cosine':\n",
    "        T_total = epochs * nBatch\n",
    "        T_cur = (epoch % epochs) * nBatch + batch\n",
    "        lr = 0.5 * init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n",
    "    elif method == 'multistep':\n",
    "        data = \"cifar100\"\n",
    "        if data.startswith('cifar'):\n",
    "            lr, decay_rate = init_lr, 0.1\n",
    "            if epoch >= epochs * 0.75:\n",
    "                lr *= decay_rate ** 2\n",
    "            elif epoch >= epochs * 0.5:\n",
    "                lr *= decay_rate\n",
    "        else:\n",
    "            lr = init_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0f8e69-1a3b-42da-bc29-2b5df2aa34d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtrain_acc1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mval_acc1\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtrain_acc5\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mval_acc5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc1, train_acc5, lr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkd_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43minit_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     val_loss, val_acc1, val_acc5 \u001b[38;5;241m=\u001b[39m validate(testloader, model, kd_loss)\n",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, kd_loss, optimizer, epoch, epochs, init_lr, print_freq)\u001b[0m\n\u001b[1;32m     54\u001b[0m target_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(target)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m output, soft_target \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     59\u001b[0m     output \u001b[38;5;241m=\u001b[39m [output]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/notebooks/fss/dns_msdnet.py:436\u001b[0m, in \u001b[0;36mMSDNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 进行特征分割和特征重组，最后一层不能分割重组\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnBlocks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: \n\u001b[0;32m--> 436\u001b[0m     x, x_sub \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_reroute_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdns_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[1;32m    438\u001b[0m     x_sub \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/notebooks/fss/dns_msdnet.py:28\u001b[0m, in \u001b[0;36mfeature_reroute_func\u001b[0;34m(x, ratio)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_reroute_func\u001b[39m(x, ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureReroute(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m1\u001b[39m), ratio)(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "scores = ['epoch\\tlr\\ttrain_loss\\tval_loss\\ttrain_acc1'\n",
    "              '\\tval_acc1\\ttrain_acc5\\tval_acc5']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    train_loss, train_acc1, train_acc5, lr = train(trainloader, model, kd_loss, optimizer, epoch,epochs,init_lr)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_acc1, val_acc5 = validate(testloader, model, kd_loss)\n",
    "\n",
    "    # save scores to a tsv file, rewrite the whole file to prevent\n",
    "    # accidental deletion\n",
    "    scores.append(('{}\\t{:.3f}' + '\\t{:.4f}' * 6)\n",
    "                  .format(epoch, lr, train_loss, val_loss,\n",
    "                          train_acc1, val_acc1, train_acc5, val_acc5))\n",
    "\n",
    "    is_best = val_acc1 > best_acc1\n",
    "    if is_best:\n",
    "        best_acc1 = val_acc1\n",
    "        best_epoch = epoch\n",
    "        print('Best var_acc1 {}'.format(best_acc1))\n",
    "\n",
    "    save_path = f\"./models/{model_name}-cifar100/\"\n",
    "    model_filename = 'checkpoint_%03d.pth.tar' % epoch\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, save_path, is_best, model_filename, scores)\n",
    "\n",
    "print('Best val_acc1: {:.4f} at epoch {}'.format(best_acc1, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8fe97-e117-4d95-9398-f8b1b374d6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
